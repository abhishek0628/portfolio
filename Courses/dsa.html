<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Data Structure Basics</title>
<style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    ul, ol { margin-bottom: 15px; }
    table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }
    table, th, td { border: 1px solid #ccc; }
    th, td { padding: 8px; text-align: left; }
    th { background-color: #f5f5f5; }
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }
    table, th, td { border: 1px solid #ccc; }
    th, td { padding: 8px; text-align: left; }
    th { background-color: #f5f5f5; }
    pre { background-color: #f4f4f4; padding: 10px; border-left: 4px solid #3498db; overflow-x: auto; }
    code { font-family: monospace; }
</style>
</head>
<!-- <body>
    <style>
    body { font-family: Arial, sans-serif; line-height: 1.6; margin: 20px; }
    h1, h2, h3 { color: #2c3e50; }
    ul, ol { margin-bottom: 15px; }
    table { width: 100%; border-collapse: collapse; margin-bottom: 20px; }
    table, th, td { border: 1px solid #ccc; }
    th, td { padding: 8px; text-align: left; }
    th { background-color: #f5f5f5; }
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 4px; }
</style> -->

<h1>üìö What is Data Structure?</h1>
<p>A <strong>data structure</strong> is a way to store and organize data so that it can be used efficiently.</p>
<p>Think of it as a <em>container</em> that holds data in a specific layout.</p>

<h2>Example:</h2>
<ul>
    <li><strong>Array:</strong> Stores elements linearly</li>
    <li><strong>Tree:</strong> Stores elements hierarchically</li>
</ul>
<h1>üß± Primitive Data Structures</h1>
<p><strong>Definition:</strong> Primitive data structures are the basic building blocks for storing data. They are predefined by the programming language and store a single value at a time.</p>

<h2>Types of Primitive Data Structures</h2>

<h3>1. Integer (<code>int</code>)</h3>
<ul>
<li>Stores whole numbers (positive, negative, or zero)</li>
<li>Examples: <code>-5, 0, 42</code></li>
<li>Memory: Typically 4 bytes (depends on language/architecture)</li>
<li>Operations: <code>+, -, *, /, %</code></li>
</ul>

<h3>2. Floating-point (<code>float, double</code>)</h3>
<ul>
<li>Stores real numbers (numbers with fractional parts)</li>
<li>Examples: <code>3.14, -0.001, 2.0</code></li>
<li>Memory: float ‚Üí 4 bytes, double ‚Üí 8 bytes</li>
<li>Operations: Arithmetic, comparison</li>
</ul>

<h3>3. Character (<code>char</code>)</h3>
<ul>
<li>Stores a single character</li>
<li>Examples: <code>'a', 'Z', '7', '@'</code></li>
<li>Memory: Usually 1 byte (ASCII), can be 2 bytes (Unicode)</li>
<li>Operations: Comparison, concatenation (in some languages)</li>
</ul>

<h3>4. Boolean (<code>bool</code>)</h3>
<ul>
<li>Stores only <code>True</code> or <code>False</code></li>
<li>Used for conditional operations</li>
<li>Memory: Typically 1 byte</li>
<li>Operations: AND, OR, NOT (<code>&&, ||, !</code>)</li>
</ul>

<h3>5. Other Types (Language-specific)</h3>
<ul>
<li>String: Sometimes considered primitive (e.g., in Python), stores sequence of characters</li>
<li>Byte/Short/Long: Variants of integer for memory efficiency</li>
<li>Complex: In some languages, stores complex numbers (real + imaginary)</li>
</ul>

<h2>Key Points</h2>
<ul>
<li>Primitive ‚Üí Basic, single-value storage</li>
<li>Non-primitive ‚Üí Built using primitive types (like arrays, lists, structures)</li>
<li>Operations ‚Üí Arithmetic, logical, comparison, assignment</li>
</ul>

<h1>üìö Non-Primitive Data Structures</h1>
<p>Non-primitive data structures are built using primitive types (like int, float, char) to organize data efficiently. They are divided into <strong>Linear</strong> and <strong>Non-Linear</strong> structures.</p>

<h2>1Ô∏è‚É£ Linear Data Structures</h2>
<p>Elements are stored sequentially, one after another. Traversal is usually simple and sequential.</p>

<table>
<tr>
<th>Data Structure</th>
<th>Description</th>
<th>Operations</th>
</tr>
<tr>
<td>Array</td>
<td>Fixed-size, contiguous memory storage</td>
<td>Access, Insert, Delete, Traverse</td>
</tr>
<tr>
<td>Linked List</td>
<td>Nodes connected via pointers</td>
<td>Insert, Delete, Traverse</td>
</tr>
<tr>
<td>Stack</td>
<td>LIFO (Last In First Out)</td>
<td>Push, Pop, Peek</td>
</tr>
<tr>
<td>Queue</td>
<td>FIFO (First In First Out)</td>
<td>Enqueue, Dequeue, Peek</td>
</tr>
</table>

<p><strong>Key Idea:</strong> Linear structures are easy to traverse but may have limitations in dynamic memory handling (e.g., array resizing).</p>

<h2>2Ô∏è‚É£ Non-Linear Data Structures</h2>
<p>Elements are not stored sequentially; they have hierarchical or interconnected relationships. Traversal can be more complex.</p>

<table>
<tr>
<th>Data Structure</th>
<th>Description</th>
<th>Use Cases</th>
</tr>
<tr>
<td>Tree</td>
<td>Hierarchical structure with root and child nodes</td>
<td>Expression parsing, File systems</td>
</tr>
<tr>
<td>Graph</td>
<td>Nodes (vertices) connected by edges</td>
<td>Networks, Social media, Routing algorithms</td>
</tr>
<tr>
<td>Hash Table</td>
<td>Key-value mapping using hash function</td>
<td>Fast lookups, Databases, Caches</td>
</tr>
</table>

<p><strong>Key Idea:</strong> Non-linear structures are flexible for complex relationships and allow efficient searching, insertion, and deletion in large datasets.</p>
<h1>üìå What is an Array?</h1>
<p>An array is a collection of elements of the same type stored in contiguous memory locations. Each element can be accessed using its index (starting from 0).</p>

<h3>Example:</h3>
<pre><code>int arr[5] = {2, 4, 6, 8, 10};

// Accessing elements
arr[0] = 2
arr[1] = 4
arr[4] = 10
</code></pre>

<h2>2. Types of Arrays</h2>

<h3>One-Dimensional Array (1D)</h3>
<p>A single row of elements.</p>
<pre><code>Example: [2, 4, 6, 8]</code></pre>

<h3>Two-Dimensional Array (2D)</h3>
<p>Elements arranged in rows and columns.</p>
<pre><code>int matrix[2][3] = {
    {1, 2, 3},
    {4, 5, 6}
};

// Accessing an element
matrix[1][2] = 6
</code></pre>

<h3>Multi-Dimensional Array</h3>
<p>3D or higher-dimensional arrays, used in advanced computations.</p>

<h2>3. Array Operations</h2>

<h3>a) Traversal</h3>
<p>Access each element in the array.</p>
<pre><code>for(int i = 0; i < 5; i++) {
    printf("%d ", arr[i]);
}</code></pre>

<h3>b) Insertion</h3>
<p>Add an element at a specific position (requires shifting elements).</p>
<pre><code>Insert 7 at index 2 in arr = [2, 4, 6, 8]
Result: [2, 4, 7, 6, 8]
Time Complexity: O(n)
</code></pre>

<h3>c) Deletion</h3>
<p>Remove an element at a specific position (requires shifting elements).</p>
<pre><code>Delete index 1 from arr = [2, 4, 6, 8]
Result: [2, 6, 8]
Time Complexity: O(n)
</code></pre>

<h3>d) Searching</h3>
<ul>
    <li>Linear Search ‚Üí Check each element ‚Üí O(n)</li>
    <li>Binary Search ‚Üí Array must be sorted ‚Üí O(log n)</li>
</ul>

<h3>e) Update</h3>
<p>Change the value at a given index.</p>
<pre><code>arr[2] = 10; // Changes third element to 10
</code></pre>

<h2>4. Advantages of Arrays</h2>
<ul>
    <li>Fast access using index (O(1))</li>
    <li>Simple to implement</li>
    <li>Useful for storing large numbers of elements of the same type</li>
</ul>

<h2>5. Disadvantages of Arrays</h2>
<ul>
    <li>Fixed size (static arrays)</li>
    <li>Insertion and deletion are costly (O(n))</li>
    <li>Wastes memory if array size is overestimated</li>
</ul>

<h2>6. Important Variants</h2>
<ul>
    <li><strong>Dynamic Array:</strong> Can grow/shrink at runtime (e.g., vector in C++ or ArrayList in Java)</li>
    <li><strong>Sparse Array:</strong> Most elements are 0 or NULL, stored efficiently</li>
</ul>
<h1>üîó Linked List Overview</h1>

<h2>1. What is a Linked List?</h2>
<p>A linked list is a linear data structure where elements (called <strong>nodes</strong>) are stored non-contiguously and connected via pointers.</p>

<p>Each node has:</p>
<ul>
<li><strong>Data</strong> ‚Äì the value stored.</li>
<li><strong>Pointer (next)</strong> ‚Äì address of the next node.</li>
</ul>

<h3>Example Node Structure (C)</h3>
<pre><code>struct Node {
    int data;
    struct Node* next;
};</code></pre>

<h2>2. Types of Linked Lists</h2>
<table>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
<tr>
<td>Singly Linked List (SLL)</td>
<td>Each node points to the next node. Last node points to NULL.</td>
</tr>
<tr>
<td>Doubly Linked List (DLL)</td>
<td>Each node has <code>next</code> and <code>prev</code> pointers. Traversable forward and backward.</td>
</tr>
<tr>
<td>Circular Linked List (CLL)</td>
<td>Last node points back to the first node, forming a circle. Can be singly or doubly linked.</td>
</tr>
</table>

<h2>3. Basic Operations</h2>

<h3>a) Traversal</h3>
<p>Visit each node from head to end. <strong>Time Complexity:</strong> O(n)</p>
<pre><code>void traverse(struct Node* head) {
    struct Node* temp = head;
    while(temp != NULL) {
        printf("%d ", temp->data);
        temp = temp->next;
    }
}</code></pre>

<h3>b) Insertion</h3>
<ul>
<li><strong>At Beginning:</strong> New node points to current head, update head. O(1)</li>
<li><strong>At End:</strong> Traverse to last node, make its next point to new node. O(n)</li>
<li><strong>At Position:</strong> Traverse to position, adjust pointers. O(n)</li>
</ul>

<h3>c) Deletion</h3>
<ul>
<li><strong>From Beginning:</strong> Update head to next node, free old head. O(1)</li>
<li><strong>From End:</strong> Traverse to second-last node, set its next to NULL, free last node. O(n)</li>
<li><strong>From Position:</strong> Traverse, adjust pointers, free node. O(n)</li>
</ul>

<h3>d) Searching</h3>
<p>Linear search through nodes. <strong>Time Complexity:</strong> O(n)</p>
<pre><code>int search(struct Node* head, int key) {
    struct Node* temp = head;
    while(temp != NULL) {
        if(temp->data == key) return 1; // found
        temp = temp->next;
    }
    return 0; // not found
}</code></pre>

<h2>4. Advantages of Linked Lists</h2>
<ul>
<li>Dynamic memory allocation</li>
<li>Efficient insertion/deletion at beginning or middle</li>
<li>No predefined size required (unlike arrays)</li>
</ul>

<h3>Disadvantages:</h3>
<ul>
<li>Extra memory for pointer</li>
<li>No direct access (O(n) to access ith element)</li>
<li>Traversal slower than arrays</li>
</ul>

<h2>5. Variants in Use</h2>
<ul>
<li><strong>Stack:</strong> Can be implemented using SLL (push/pop at head)</li>
<li><strong>Queue:</strong> SLL/DLL for enqueue/dequeue</li>
<li><strong>Graph adjacency list:</strong> Uses linked lists for neighbors</li>
<li><strong>Polynomial representation:</strong> Each term as a node</li>
</ul>

<h1>üìö Stack Data Structure</h1>

<h2>1. What is a Stack?</h2>
<p>
A <strong>stack</strong> is a linear data structure that follows the <strong>LIFO (Last In First Out)</strong> principle.
</p>
<p><strong>LIFO:</strong> The last element added is the first one to be removed.</p>
<p><strong>Analogy:</strong> A stack of plates ‚Äî you put plates on top and remove from the top.</p>

<h2>2. Basic Operations</h2>
<table>
<tr>
<th>Operation</th>
<th>Description</th>
<th>Time Complexity</th>
</tr>
<tr>
<td><code>push(x)</code></td>
<td>Insert element <code>x</code> at the top</td>
<td>O(1)</td>
</tr>
<tr>
<td><code>pop()</code></td>
<td>Remove the top element</td>
<td>O(1)</td>
</tr>
<tr>
<td><code>peek()</code> / <code>top()</code></td>
<td>Get the top element without removing</td>
<td>O(1)</td>
</tr>
<tr>
<td><code>isEmpty()</code></td>
<td>Check if stack is empty</td>
<td>O(1)</td>
</tr>
<tr>
<td><code>size()</code></td>
<td>Number of elements in the stack</td>
<td>O(1)</td>
</tr>
</table>

<h2>3. Stack Representation</h2>
<ul>
<li><strong>Using Array</strong>: Fixed size (static stack). Top index is maintained.</li>
<li><strong>Using Linked List</strong>: Dynamic size (no fixed limit). Each node points to the next.</li>
<li><strong>Using STL / Library</strong>: High-level implementation (e.g., <code>stack&lt;int&gt; s;</code> in C++).</li>
</ul>

<h2>4. Example Usage</h2>
<ul>
<li>Undo/Redo in text editors</li>
<li>Expression evaluation (infix, postfix, prefix)</li>
<li>Function call stack in programming</li>
<li>Backtracking (Maze solving, N-Queens problem)</li>
</ul>

<h2>5. Example: Stack Operations (Array Implementation)</h2>
<pre><code>#include &lt;stdio.h&gt;
#define MAX 5

int stack[MAX];
int top = -1;

void push(int x) {
    if(top == MAX-1) {
        printf("Stack Overflow\n");
    } else {
        stack[++top] = x;
        printf("%d pushed\n", x);
    }
}

void pop() {
    if(top == -1) {
        printf("Stack Underflow\n");
    } else {
        printf("%d popped\n", stack[top--]);
    }
}

void peek() {
    if(top == -1) printf("Stack is empty\n");
    else printf("Top element: %d\n", stack[top]);
}

int main() {
    push(10);
    push(20);
    peek();
    pop();
    peek();
    return 0;
}
</code></pre>

<h3>Output:</h3>
<pre><code>10 pushed
20 pushed
Top element: 20
20 popped
Top element: 10
</code></pre>

<h3>Using Linked List (Dynamic Stack)</h3>
<ul>
<li>Each node has <code>data</code> and <code>next</code>.</li>
<li><code>push</code> ‚Üí add node at head.</li>
<li><code>pop</code> ‚Üí remove node from head.</li>
</ul>

<h2>6. Stack Applications in Algorithms</h2>
<ul>
<li>Reverse a string</li>
<li>Check balanced parentheses ( <code>()</code>, <code>{}</code>, <code>[]</code> )</li>
<li>Depth First Search (DFS) in graphs</li>
<li>Expression evaluation & conversion (infix ‚Üí postfix)</li>
</ul>

<h1>üßÆ Queue ‚Äì Data Structure</h1>

<h2>1. What is a Queue?</h2>
<p>A <strong>Queue</strong> is a linear data structure that follows the <strong>FIFO (First In, First Out)</strong> principle.</p>
<p>üëâ The first element inserted into the queue is the first one to be removed.</p>
<p>You can think of it like a real-life queue ‚Äî the first person in line is the first to get served.</p>

<h2>üß© 2. Basic Operations</h2>
<table>
<tr><th>Operation</th><th>Description</th><th>Time Complexity</th></tr>
<tr><td><code>enqueue(x)</code></td><td>Insert element <code>x</code> at the rear of the queue</td><td>O(1)</td></tr>
<tr><td><code>dequeue()</code></td><td>Remove element from the front of the queue</td><td>O(1)</td></tr>
<tr><td><code>peek()</code> / <code>front()</code></td><td>Get the front element without removing it</td><td>O(1)</td></tr>
<tr><td><code>isEmpty()</code></td><td>Check if queue is empty</td><td>O(1)</td></tr>
<tr><td><code>isFull()</code></td><td>(for fixed-size queue) Check if queue is full</td><td>O(1)</td></tr>
</table>

<h2>‚öôÔ∏è 3. Structure of a Queue</h2>
<pre>Front ‚Üí [10] [20] [30] ‚Üê Rear</pre>
<p><strong>Front</strong> ‚Üí points to the element to be dequeued next.<br>
<strong>Rear</strong> ‚Üí points to the position where the next element will be enqueued.</p>

<h2>üß† 4. Implementation Methods</h2>

<h3>a) Using Array</h3>
<p>Fixed size ‚Äî may cause overflow if the rear reaches the maximum size.</p>

<pre><code>#include &lt;stdio.h&gt;
#define SIZE 5

int queue[SIZE];
int front = -1, rear = -1;

void enqueue(int x) {
    if (rear == SIZE - 1)
        printf("Queue Overflow\n");
    else {
        if (front == -1) front = 0;
        queue[++rear] = x;
        printf("%d inserted\n", x);
    }
}

void dequeue() {
    if (front == -1 || front > rear)
        printf("Queue Underflow\n");
    else
        printf("%d deleted\n", queue[front++]);
}

void display() {
    if (front == -1)
        printf("Queue is empty\n");
    else {
        printf("Queue: ");
        for (int i = front; i <= rear; i++)
            printf("%d ", queue[i]);
        printf("\n");
    }
}

int main() {
    enqueue(10);
    enqueue(20);
    enqueue(30);
    display();
    dequeue();
    display();
    return 0;
}</code></pre>

<h3>b) Using Linked List</h3>
<p>Dynamic size ‚Äî no overflow (unless memory is full). Each node contains <strong>data + pointer</strong> to the next node.</p>

<h2>üîÅ 5. Variants of Queue</h2>
<table>
<tr><th>Type</th><th>Description</th><th>Example</th></tr>
<tr><td>Simple Queue</td><td>FIFO ‚Äî normal queue</td><td>Printer queue</td></tr>
<tr><td>Circular Queue</td><td>Last position connects back to first</td><td>CPU scheduling</td></tr>
<tr><td>Priority Queue</td><td>Elements dequeued by priority</td><td>Dijkstra‚Äôs algorithm</td></tr>
<tr><td>Deque (Double-Ended Queue)</td><td>Insertion & deletion at both ends</td><td>Palindrome checking</td></tr>
</table>

<h2>üí° 6. Example Operations (Circular Queue)</h2>
<table>
<tr><th>Operation</th><th>Result</th></tr>
<tr><td>Enqueue(10)</td><td>[10]</td></tr>
<tr><td>Enqueue(20)</td><td>[10, 20]</td></tr>
<tr><td>Dequeue()</td><td>Removes 10</td></tr>
<tr><td>Enqueue(30)</td><td>[20, 30]</td></tr>
<tr><td>Enqueue(40)</td><td>[20, 30, 40]</td></tr>
<tr><td>Enqueue(50)</td><td>[20, 30, 40, 50]</td></tr>
</table>
<p>When the queue reaches the end of the array, the <strong>rear wraps around</strong>.</p>

<h2>üöÄ 7. Real-World Applications</h2>
<ul>
<li>CPU Scheduling</li>
<li>Disk Scheduling</li>
<li>Printer Job Queue</li>
<li>Call Center Systems</li>
<li>BFS (Breadth-First Search) in Graphs</li>
<li>LRU Cache (Caching Mechanism)</li>
</ul>

<h2>üßÆ 8. Time & Space Complexity</h2>
<table>
<tr><th>Operation</th><th>Time</th><th>Space</th></tr>
<tr><td>Enqueue</td><td>O(1)</td><td>O(n)</td></tr>
<tr><td>Dequeue</td><td>O(1)</td><td>O(n)</td></tr>
<tr><td>Peek</td><td>O(1)</td><td>O(1)</td></tr>
</table>
<h1>üå≥ What is a Tree?</h1>
<p>A <strong>tree</strong> is a non-linear hierarchical data structure made up of <strong>nodes</strong>, where:</p>
<ul>
    <li>The topmost node is called the <strong>root</strong>.</li>
    <li>Each node contains data and links (edges) to its child nodes.</li>
    <li>A tree has no cycles ‚Äî every child has exactly one parent.</li>
</ul>

<div class="section">
<h2>Basic Terminology</h2>
<table>
<tr><th>Term</th><th>Meaning</th></tr>
<tr><td>Root</td><td>The topmost node (no parent).</td></tr>
<tr><td>Parent</td><td>Node with one or more child nodes.</td></tr>
<tr><td>Child</td><td>Node directly descended from a parent.</td></tr>
<tr><td>Siblings</td><td>Nodes with the same parent.</td></tr>
<tr><td>Leaf Node</td><td>Node with no children.</td></tr>
<tr><td>Edge</td><td>Connection between parent and child.</td></tr>
<tr><td>Level</td><td>Distance (in edges) from the root.</td></tr>
<tr><td>Height of Tree</td><td>Length of the longest path from root to a leaf.</td></tr>
<tr><td>Degree</td><td>Number of children a node has.</td></tr>
</table>
</div>

<div class="section">
<h2>Example Tree</h2>
<pre class="example-tree">
        A
       / \
      B   C
     / \   \
    D   E   F
</pre>
<ul>
<li>Root = A</li>
<li>Children of A = {B, C}</li>
<li>Parent of D = B</li>
<li>Leaf nodes = {D, E, F}</li>
<li>Height = 2</li>
</ul>
</div>

<div class="section">
<h2>Types of Trees</h2>

<h3>üåø 1. General Tree</h3>
<p>Each node can have any number of children. No fixed structure.</p>

<h3>üåø 2. Binary Tree</h3>
<p>Each node can have at most two children ‚Äî left and right.</p>
<p><strong>Traversal Methods:</strong></p>
<ul>
<li>Inorder (Left, Root, Right)</li>
<li>Preorder (Root, Left, Right)</li>
<li>Postorder (Left, Right, Root)</li>
<li>Level Order (Breadth-first)</li>
</ul>

<pre class="example-tree">
     10
    /  \
   5    15
</pre>

<table>
<tr><th>Traversal Type</th><th>Output</th></tr>
<tr><td>Preorder</td><td>10 5 15</td></tr>
<tr><td>Inorder</td><td>5 10 15</td></tr>
<tr><td>Postorder</td><td>5 15 10</td></tr>
</table>

<h3>üåø 3. Binary Search Tree (BST)</h3>
<p>A binary tree with the property: <code>Left subtree &lt; root &lt; Right subtree</code></p>

<pre class="example-tree">
     8
    / \
   3  10
  / \   \
 1  6    14
</pre>

<table>
<tr><th>Operation</th><th>Time Complexity (Average)</th></tr>
<tr><td>Search</td><td>O(log n)</td></tr>
<tr><td>Insert</td><td>O(log n)</td></tr>
<tr><td>Delete</td><td>O(log n)</td></tr>
</table>

<h3>üåø 4. Balanced Binary Trees</h3>
<p>Maintain minimum height for faster operations.</p>
<ul>
<li><strong>AVL Tree:</strong> Balance factor = -1, 0, or +1</li>
<li><strong>Red-Black Tree:</strong> Balances using color properties</li>
<li><strong>B-Tree / B+ Tree:</strong> Used in databases and file systems</li>
</ul>

<h3>üåø 5. Binary Heap</h3>
<p>A <strong>complete binary tree</strong> used in priority queues.</p>
<ul>
<li><strong>Min-Heap:</strong> Parent ‚â§ Children</li>
<li><strong>Max-Heap:</strong> Parent ‚â• Children</li>
</ul>

<h3>üåø 6. Trie (Prefix Tree)</h3>
<p>Used for string searching and autocomplete systems. Each node represents a character.</p>

<pre class="example-tree">
Words: {"cat", "can", "cap"}

      (root)
      / 
     c
      \
       a
     / | \
    t  n  p
</pre>

<h3>üåø 7. N-ary Tree</h3>
<p>Each node can have up to N children ‚Äî a generalized tree structure.</p>
</div>

<div class="section">
<h2>Tree Traversal Example in C</h2>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

struct Node {
    int data;
    struct Node* left;
    struct Node* right;
};

struct Node* newNode(int data) {
    struct Node* node = (struct Node*)malloc(sizeof(struct Node));
    node-&gt;data = data;
    node-&gt;left = node-&gt;right = NULL;
    return node;
}

void inorder(struct Node* root) {
    if (root != NULL) {
        inorder(root-&gt;left);
        printf("%d ", root-&gt;data);
        inorder(root-&gt;right);
    }
}

int main() {
    struct Node* root = newNode(10);
    root-&gt;left = newNode(5);
    root-&gt;right = newNode(15);
    root-&gt;left-&gt;left = newNode(2);
    root-&gt;left-&gt;right = newNode(7);

    printf("Inorder Traversal: ");
    inorder(root);
}
</code></pre>

<p><strong>Output:</strong></p>
<pre>Inorder Traversal: 2 5 7 10 15</pre>
</div>

<div class="section">
<h2>Applications of Trees</h2>
<table>
<tr><th>Application</th><th>Example</th></tr>
<tr><td>File Systems</td><td>Directory hierarchy</td></tr>
<tr><td>Databases</td><td>B-trees, B+ trees</td></tr>
<tr><td>AI</td><td>Decision trees</td></tr>
<tr><td>Networking</td><td>Routing tables</td></tr>
<tr><td>Compiler</td><td>Abstract Syntax Tree</td></tr>
<tr><td>Web</td><td>DOM (Document Object Model)</td></tr>
</table>
</div>

<h1>üß† 1. What is a Graph?</h1>
<p>
A <strong>graph</strong> is a non-linear data structure consisting of:
</p>
<ul>
    <li><strong>Vertices (nodes):</strong> The entities in the graph</li>
    <li><strong>Edges (links):</strong> The connections between those entities</li>
</ul>

<p><strong>Formally:</strong></p>
<p><code>G = (V, E)</code></p>
<ul>
    <li><code>V</code> = set of vertices</li>
    <li><code>E</code> = set of edges</li>
</ul>

<hr>

<h2>üîó 2. Types of Graphs</h2>

<h3>a) Based on Direction</h3>
<table>
<tr><th>Type</th><th>Description</th><th>Example</th></tr>
<tr><td>Undirected Graph</td><td>Edges have no direction</td><td>Friendship network</td></tr>
<tr><td>Directed Graph (Digraph)</td><td>Edges have direction</td><td>Twitter follower network</td></tr>
</table>

<h3>b) Based on Weight</h3>
<table>
<tr><th>Type</th><th>Description</th><th>Example</th></tr>
<tr><td>Weighted Graph</td><td>Each edge has a weight or cost</td><td>Road distances, flight fares</td></tr>
<tr><td>Unweighted Graph</td><td>All edges are equal</td><td>Simple connectivity maps</td></tr>
</table>

<h3>c) Based on Structure</h3>
<table>
<tr><th>Type</th><th>Description</th></tr>
<tr><td>Cyclic Graph</td><td>Contains at least one cycle</td></tr>
<tr><td>Acyclic Graph</td><td>Contains no cycles</td></tr>
<tr><td>Connected Graph</td><td>Every vertex is reachable from any other</td></tr>
<tr><td>Disconnected Graph</td><td>At least one vertex is unreachable</td></tr>
<tr><td>Complete Graph</td><td>Every vertex connects to every other vertex</td></tr>
<tr><td>Bipartite Graph</td><td>Vertices divided into two disjoint sets; edges only connect across sets</td></tr>
</table>

<hr>

<h2>‚öôÔ∏è 3. Representation of Graphs</h2>

<h3>a) Adjacency Matrix</h3>
<p>2D array where <code>matrix[i][j] = 1</code> (or weight) if an edge exists between i and j.</p>
<p><strong>Space:</strong> O(V¬≤)</p>

<pre>
   0 1 2 3
0 [0 1 0 1]
1 [1 0 1 0]
2 [0 1 0 1]
3 [1 0 1 0]
</pre>

<h3>b) Adjacency List</h3>
<p>Each vertex stores a list of adjacent vertices.</p>
<p><strong>Space:</strong> O(V + E)</p>

<pre>
0 ‚Üí 1, 3
1 ‚Üí 0, 2
2 ‚Üí 1, 3
3 ‚Üí 0, 2
</pre>

<hr>

<h2>üß≠ 4. Graph Traversal Algorithms</h2>

<h3>a) Breadth-First Search (BFS)</h3>
<ul>
    <li>Level-wise traversal (uses a queue)</li>
    <li>Good for finding shortest path in unweighted graphs</li>
</ul>
<p><strong>Time Complexity:</strong> O(V + E)</p>

<pre><code>
void BFS(int start) {
    int visited[MAX] = {0};
    Queue q;
    enqueue(&q, start);
    visited[start] = 1;

    while (!isEmpty(&q)) {
        int v = dequeue(&q);
        printf("%d ", v);
        for (int i = 0; i < n; i++) {
            if (adj[v][i] && !visited[i]) {
                enqueue(&q, i);
                visited[i] = 1;
            }
        }
    }
}
</code></pre>

<h3>b) Depth-First Search (DFS)</h3>
<ul>
    <li>Goes as deep as possible along each branch before backtracking</li>
    <li>Uses stack (recursion or iterative)</li>
</ul>
<p><strong>Time Complexity:</strong> O(V + E)</p>

<pre><code>
void DFS(int v, int visited[]) {
    visited[v] = 1;
    printf("%d ", v);

    for (int i = 0; i < n; i++) {
        if (adj[v][i] && !visited[i])
            DFS(i, visited);
    }
}
</code></pre>

<hr>

<h2>üßÆ 5. Important Graph Algorithms</h2>

<table>
<tr><th>Algorithm</th><th>Purpose</th><th>Time Complexity</th></tr>
<tr><td>BFS / DFS</td><td>Traversal / Connectivity</td><td>O(V + E)</td></tr>
<tr><td>Dijkstra‚Äôs Algorithm</td><td>Shortest path (no negative weights)</td><td>O(V¬≤) / O(E log V)</td></tr>
<tr><td>Bellman‚ÄìFord Algorithm</td><td>Shortest path (handles negative weights)</td><td>O(VE)</td></tr>
<tr><td>Floyd‚ÄìWarshall</td><td>All-pairs shortest path</td><td>O(V¬≥)</td></tr>
<tr><td>Kruskal‚Äôs Algorithm</td><td>Minimum Spanning Tree (MST)</td><td>O(E log E)</td></tr>
<tr><td>Prim‚Äôs Algorithm</td><td>Minimum Spanning Tree (MST)</td><td>O(V¬≤) / O(E log V)</td></tr>
<tr><td>Topological Sort</td><td>Ordering in DAG</td><td>O(V + E)</td></tr>
<tr><td>Tarjan / Kosaraju Algorithm</td><td>Strongly Connected Components (SCCs)</td><td>O(V + E)</td></tr>
</table>

<hr>

<h2>üåç 6. Real-Life Applications</h2>
<table>
<tr><th>Application</th><th>Example</th></tr>
<tr><td>Social Networks</td><td>Users as nodes, friendships as edges</td></tr>
<tr><td>Navigation Systems</td><td>Cities as nodes, roads as edges</td></tr>
<tr><td>Internet (Routing)</td><td>Routers as nodes, connections as edges</td></tr>
<tr><td>Recommendation Systems</td><td>User‚Äìitem relationships</td></tr>
<tr><td>Computer Networks</td><td>Used in OSPF, RIP routing algorithms</td></tr>
</table>

<hr>

<h2>üß© 7. Visual Example</h2>
<p>For this undirected weighted graph:</p>

<pre>
   (1)
  /   \
(0)---(2)
  \   /
   (3)
</pre>

<p><strong>Adjacency List Representation:</strong></p>
<pre>
0 ‚Üí 1(2), 2(4), 3(5)
1 ‚Üí 0(2), 2(1)
2 ‚Üí 0(4), 1(1), 3(3)
3 ‚Üí 0(5), 2(3)
</pre>
<h1>üîπ Shortest Path Algorithms</h1>

<h2>1. Types of Graphs for Shortest Path</h2>

<h3>Unweighted Graph:</h3>
<ul>
<li>Each edge has equal weight (e.g., 1).</li>
<li><span class="highlight">Use BFS (Breadth-First Search)</span></li>
</ul>

<h3>Weighted Graph:</h3>
<ul>
<li>Each edge has a weight (cost, distance, or time).</li>
<li>Use algorithms like:
    <ul>
        <li><strong>Dijkstra‚Äôs Algorithm</strong> ‚Äì For graphs with <em>no negative weights</em></li>
        <li><strong>Bellman-Ford Algorithm</strong> ‚Äì Handles <em>negative weights</em></li>
        <li><strong>Floyd‚ÄìWarshall Algorithm</strong> ‚Äì For <em>all-pairs shortest paths</em></li>
    </ul>
</li>
</ul>

<hr>

<h2>2. Algorithms</h2>

<div class="algo-box">
<h3>(a) BFS (for Unweighted Graphs)</h3>
<p><strong>Purpose:</strong> Finds the shortest path by visiting nodes level by level.</p>
<p><strong>Time Complexity:</strong> O(V + E)</p>

<p><strong>Example (unweighted):</strong></p>
<pre>
A - B - C
|       |
D-------E
</pre>
<p><strong>Shortest path A ‚Üí E:</strong> A‚ÄìD‚ÄìE (2 edges)</p>
</div>

<div class="algo-box">
<h3>(b) Dijkstra‚Äôs Algorithm</h3>
<p><strong>Used for:</strong> Weighted graphs <em>without negative weights</em>.</p>

<p><strong>Idea:</strong></p>
<ol>
<li>Start from the source vertex.</li>
<li>Keep track of the shortest known distance to all vertices.</li>
<li>Update distances when shorter paths are found.</li>
<li>Continue until all vertices are processed.</li>
</ol>

<p><strong>Time Complexity:</strong></p>
<ul>
<li>Adjacency matrix: O(V¬≤)</li>
<li>Priority queue (heap): O((V + E) log V)</li>
</ul>

<p><strong>Steps:</strong></p>
<ol>
<li>Initialize all distances as ‚àû, except source = 0</li>
<li>Pick vertex with smallest distance (using priority queue)</li>
<li>Update its neighbors‚Äô distances</li>
<li>Repeat until all vertices are visited</li>
</ol>

<p><strong>Example:</strong></p>
<pre>
A --1-- B --3-- D
|      /
4     2
|    /
C
</pre>

<table>
<tr><th>Vertex</th><th>Distance from A</th><th>Path</th></tr>
<tr><td>A</td><td>0</td><td>A</td></tr>
<tr><td>B</td><td>1</td><td>A‚ÄìB</td></tr>
<tr><td>C</td><td>3</td><td>A‚ÄìB‚ÄìC</td></tr>
<tr><td>D</td><td>4</td><td>A‚ÄìB‚ÄìD</td></tr>
</table>
</div>

<div class="algo-box">
<h3>(c) Bellman‚ÄìFord Algorithm</h3>
<p><strong>Used for:</strong> Graphs with <em>negative edge weights</em>.</p>

<p><strong>Idea:</strong> Relax all edges (V‚àí1) times (where V = number of vertices).</p>
<p>If a shorter path is found after (V‚àí1) iterations ‚Üí <strong>Negative cycle exists</strong>.</p>

<p><strong>Time Complexity:</strong> O(VE)</p>
</div>

<div class="algo-box">
<h3>(d) Floyd‚ÄìWarshall Algorithm</h3>
<p><strong>Used for:</strong> Finding shortest paths between <em>all pairs of vertices</em>.</p>
<p><strong>Idea:</strong> Dynamic Programming approach:</p>
<pre>
dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])
</pre>
<p><strong>for each vertex k.</strong></p>
<p><strong>Time Complexity:</strong> O(V¬≥)</p>
</div>

<hr>

<h2>3. When to Use Which Algorithm</h2>

<table>
<tr>
<th>Graph Type</th>
<th>Negative Weights</th>
<th>Need All-Pairs?</th>
<th>Algorithm</th>
</tr>
<tr>
<td>Unweighted</td>
<td>No</td>
<td>No</td>
<td>BFS</td>
</tr>
<tr>
<td>Weighted</td>
<td>No</td>
<td>No</td>
<td>Dijkstra</td>
</tr>
<tr>
<td>Weighted</td>
<td>Yes</td>
<td>No</td>
<td>Bellman‚ÄìFord</td>
</tr>
<tr>
<td>Weighted</td>
<td>Yes/No</td>
<td>Yes</td>
<td>Floyd‚ÄìWarshall</td>
</tr>
</table>
<h1>üåê Breadth-First Search (BFS)</h1>

<p>BFS is a graph traversal algorithm used to explore all vertices of a graph level by level, starting from a source vertex. It is mainly used in unweighted graphs to find the shortest path (by number of edges) between vertices. BFS uses a queue (FIFO) data structure.</p>

<h2>‚öôÔ∏è Algorithm Steps</h2>

<p><strong>Input:</strong> Graph G(V, E), starting vertex <code>s</code><br>
<strong>Output:</strong> Order of traversal or shortest distance from <code>s</code> to all vertices.</p>

<ol>
<li>Initialize all vertices as unvisited.</li>
<li>Create a queue and enqueue the starting vertex <code>s</code>.</li>
<li>Mark <code>s</code> as visited.</li>
<li>While the queue is not empty:
  <ul>
    <li>Dequeue a vertex <code>u</code>.</li>
    <li>For each unvisited neighbor <code>v</code> of <code>u</code>:
      <ul>
        <li>Mark <code>v</code> as visited.</li>
        <li>Enqueue <code>v</code>.</li>
      </ul>
    </li>
  </ul>
</li>
<li>Repeat until all reachable vertices are processed.</li>
</ol>

<h2>üß© Example Graph</h2>

<pre>
    A
   / \
  B   C
  |   |
  D   E
</pre>

<p><strong>Adjacency List:</strong></p>
<ul>
<li>A: B, C</li>
<li>B: A, D</li>
<li>C: A, E</li>
<li>D: B</li>
<li>E: C</li>
</ul>

<p><strong>Starting vertex:</strong> A</p>

<h3>Step-by-Step BFS Traversal</h3>
<table>
<tr>
<th>Step</th>
<th>Queue</th>
<th>Visited</th>
<th>Output</th>
</tr>
<tr><td>1</td><td>[A]</td><td>{A}</td><td>A</td></tr>
<tr><td>2</td><td>[B, C]</td><td>{A, B, C}</td><td>A</td></tr>
<tr><td>3</td><td>[C, D]</td><td>{A, B, C, D}</td><td>A, B</td></tr>
<tr><td>4</td><td>[D, E]</td><td>{A, B, C, D, E}</td><td>A, B, C</td></tr>
<tr><td>5</td><td>[E]</td><td>{A, B, C, D, E}</td><td>A, B, C, D</td></tr>
<tr><td>6</td><td>[]</td><td>{A, B, C, D, E}</td><td>A, B, C, D, E</td></tr>
</table>

<p>‚úÖ <strong>Final BFS Order:</strong> A ‚Üí B ‚Üí C ‚Üí D ‚Üí E</p>

<h2>üíª C Implementation</h2>
<pre>
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

#define MAX 100

int adj[MAX][MAX];   // Adjacency matrix
int visited[MAX];
int queue[MAX];
int front = -1, rear = -1;

void enqueue(int v) {
    if (rear == MAX - 1) return;
    if (front == -1) front = 0;
    queue[++rear] = v;
}

int dequeue() {
    if (front == -1 || front &gt; rear) return -1;
    return queue[front++];
}

void BFS(int start, int n) {
    for (int i = 0; i &lt; n; i++)
        visited[i] = 0;

    enqueue(start);
    visited[start] = 1;

    while (front &lt;= rear) {
        int v = dequeue();
        printf("%d ", v);

        for (int i = 0; i &lt; n; i++) {
            if (adj[v][i] == 1 &amp;&amp; !visited[i]) {
                enqueue(i);
                visited[i] = 1;
            }
        }
    }
}

int main() {
    int n, edges, u, v, start;

    printf("Enter number of vertices: ");
    scanf("%d", &amp;n);
    printf("Enter number of edges: ");
    scanf("%d", &amp;edges);

    for (int i = 0; i &lt; edges; i++) {
        printf("Enter edge (u v): ");
        scanf("%d%d", &amp;u, &amp;v);
        adj[u][v] = adj[v][u] = 1; // Undirected
    }

    printf("Enter starting vertex: ");
    scanf("%d", &amp;start);

    printf("BFS Traversal: ");
    BFS(start, n);

    return 0;
}
</pre>

<h2>‚è± Time and Space Complexity</h2>
<table>
<tr>
<th>Case</th>
<th>Complexity</th>
</tr>
<tr><td>Time</td><td>O(V + E) ‚Äî each vertex and edge is visited once</td></tr>
<tr><td>Space</td><td>O(V) ‚Äî for queue and visited array</td></tr>
</table>

<h2>üß≠ Applications of BFS</h2>
<ul>
<li>Shortest path in unweighted graphs</li>
<li>Web crawling</li>
<li>Peer-to-peer networks (e.g., BitTorrent)</li>
<li>GPS navigation</li>
<li>Level-order traversal of a tree</li>
</ul>
<h1>üìå Dijkstra‚Äôs Algorithm</h1>

<h2>Purpose</h2>
<p>Find the shortest path from a source node to all other nodes in a weighted graph (weights ‚â• 0).</p>
<p><strong>Important:</strong></p>
<ul>
    <li>Works only with non-negative edge weights.</li>
    <li>Graph can be directed or undirected.</li>
</ul>

<h2>How It Works (Step by Step)</h2>
<ol>
    <li><strong>Initialize distances:</strong>
        <ul>
            <li>Distance from source to itself = 0.</li>
            <li>Distance to all other nodes = ‚àû (infinity).</li>
        </ul>
    </li>
    <li>Mark all nodes as unvisited.</li>
    <li>Pick the unvisited node with the smallest distance (initially, source node).</li>
    <li>Update distances of neighboring nodes:
        <ul>
            <li>For each neighbor, check if current distance + edge weight < stored distance.</li>
            <li>If yes ‚Üí update the distance.</li>
        </ul>
    </li>
    <li>Mark the current node as visited (distance finalized).</li>
    <li>Repeat steps 3‚Äì5 until all nodes are visited.</li>
</ol>

<h2>Example</h2>
<p>Graph:</p>
<pre>
     (A)
    2/  \1
   (B)---(C)
      3
</pre>
<p><strong>Source:</strong> A</p>

<ol>
    <li>Step 1: distances: A=0, B=‚àû, C=‚àû</li>
    <li>Step 2: pick A (smallest distance)
        <ul>
            <li>Update B: 0 + 2 = 2</li>
            <li>Update C: 0 + 1 = 1</li>
        </ul>
    </li>
    <li>Step 3: pick C (smallest distance 1)
        <ul>
            <li>C‚ÜíB: 1 + 3 = 4 ‚Üí not better than current B=2</li>
        </ul>
    </li>
    <li>Step 4: pick B ‚Üí done</li>
</ol>
<p><strong>Final distances:</strong> A=0, B=2, C=1</p>

<h2>Time Complexity</h2>
<ul>
    <li>Using array: O(V<sup>2</sup>)</li>
    <li>Using min-priority queue (heap): O((V + E) log V)</li>
    <li>Where V = number of vertices, E = number of edges.</li>
</ul>

<h2>Pseudocode</h2>
<pre><code>function Dijkstra(Graph, source):
    dist[] = [‚àû, ‚àû, ..., ‚àû]
    dist[source] = 0
    visited = set()
    
    while there are unvisited nodes:
        u = node with min dist[] not in visited
        visited.add(u)
        
        for each neighbor v of u:
            if v not in visited:
                alt = dist[u] + weight(u, v)
                if alt &lt; dist[v]:
                    dist[v] = alt
    return dist[]
</code></pre>

<h1>Bellman‚ÄìFord Algorithm</h1>

<h2>Purpose</h2>
<ul>
<li>Find the shortest path from a single source to all other vertices in a weighted graph.</li>
<li>Can handle negative edge weights.</li>
<li>Can detect negative weight cycles.</li>
</ul>

<h2>Idea / Steps</h2>

<h3>1. Initialization</h3>
<ul>
<li>Set the distance to the source vertex <code>s</code> as 0.</li>
<li>Set distances to all other vertices as ‚àû.</li>
</ul>

<h3>2. Relaxation</h3>
<p>For each edge <code>(u, v)</code> with weight <code>w</code>, check if the current distance to <code>v</code> can be minimized via <code>u</code>:</p>
<pre><code>if dist[u] + w &lt; dist[v] then
    dist[v] = dist[u] + w
</code></pre>
<p>Repeat this for <code>V ‚àí 1</code> iterations (V = number of vertices).</p>

<h3>3. Negative Cycle Check</h3>
<p>After <code>V ‚àí 1</code> relaxations, go through all edges once more:</p>
<pre><code>if dist[u] + w &lt; dist[v] then
    negative weight cycle exists
</code></pre>

<h2>Time & Space Complexity</h2>
<ul>
<li>Time Complexity: O(V √ó E) ‚Üí V = vertices, E = edges</li>
<li>Space Complexity: O(V) for storing distances</li>
</ul>

<h2>Example</h2>
<table>
<tr><th>Edge</th><th>Weight</th></tr>
<tr><td>A ‚Üí B</td><td>4</td></tr>
<tr><td>A ‚Üí C</td><td>2</td></tr>
<tr><td>B ‚Üí C</td><td>-1</td></tr>
<tr><td>C ‚Üí D</td><td>2</td></tr>
<tr><td>D ‚Üí B</td><td>-2</td></tr>
</table>

<h3>Steps</h3>
<ul>
<li>Initialize distances: <code>dist[A]=0, dist[B]=‚àû, dist[C]=‚àû, dist[D]=‚àû</code></li>
<li>Relax edges 3 times (V‚àí1 = 4‚àí1 = 3)</li>
<li>Check for negative cycles</li>
</ul>

<h2>Key Points</h2>
<ul>
<li>Works even if there are negative weights.</li>
<li>Fails only if a negative weight cycle is reachable from the source.</li>
<li>Slower than Dijkstra (O(VE) vs O((V+E) log V)), but more general.</li>
</ul>

<h1>üå≥ Spanning Tree & Minimum Spanning Tree (MST)</h1>

<h2>1. Definition</h2>
<p>A <strong>Spanning Tree</strong> of a connected, undirected graph is a subgraph that:</p>
<ul>
<li>Includes all vertices of the graph.</li>
<li>Is connected (there‚Äôs a path between any two vertices).</li>
<li>Has no cycles (it‚Äôs a tree).</li>
</ul>
<p><strong>Key point:</strong> A connected graph with V vertices will have exactly V-1 edges in any spanning tree.</p>

<h2>2. Minimum Spanning Tree (MST)</h2>
<p>If the graph is weighted, a <strong>Minimum Spanning Tree (MST)</strong> is a spanning tree whose sum of edge weights is minimum.</p>
<p><strong>Importance:</strong> Useful in network design, e.g., reducing cable cost or road length.</p>

<h2>3. Algorithms to Find MST</h2>

<h3>(a) Kruskal‚Äôs Algorithm</h3>
<p><strong>Idea:</strong> Pick edges in increasing weight order, adding them if they don‚Äôt form a cycle. Uses Disjoint Set (Union-Find) for cycle detection.</p>
<ol>
<li>Sort all edges by weight.</li>
<li>Pick the smallest edge, check if it forms a cycle.</li>
<li>If no cycle, include it in MST.</li>
<li>Repeat until MST has V-1 edges.</li>
</ol>
<p><strong>Time Complexity:</strong> O(E log E) (E = number of edges)</p>

<h3>(b) Prim‚Äôs Algorithm</h3>
<p><strong>Idea:</strong> Start with a vertex and grow MST one edge at a time by picking the smallest edge connecting MST to a new vertex.</p>
<ol>
<li>Start with any vertex.</li>
<li>Pick the smallest edge connecting MST to a new vertex.</li>
<li>Include the edge and vertex.</li>
<li>Repeat until all vertices are included.</li>
</ol>
<p><strong>Time Complexity:</strong></p>
<ul>
<li>Using adjacency matrix: O(V¬≤)</li>
<li>Using min-heap/priority queue: O(E log V)</li>
</ul>

<h2>4. Properties of MST</h2>
<ul>
<li>A graph can have multiple MSTs if there are edges with equal weights.</li>
<li>MST cannot contain a cycle.</li>
<li>MST has V-1 edges for a graph with V vertices.</li>
<li>MST is unique if all edge weights are distinct.</li>
</ul>

<h2>5. Example</h2>
<p>Graph (vertices A, B, C, D):</p>
<pre class="graph">
       1
A ----- B
| \    |
4 | 2 \| 3
|     C ---- D
</pre>

<h3>Kruskal‚Äôs MST:</h3>
<p>Pick edges in increasing order: 1 (A‚ÄìB), 2 (A‚ÄìC), 3 (C‚ÄìD)</p>
<p><strong>MST edges:</strong> {A‚ÄìB, A‚ÄìC, C‚ÄìD}</p>
<p><strong>Total weight:</strong> 1 + 2 + 3 = 6</p>

<h3>Prim‚Äôs MST (starting from A):</h3>
<p>Start at A ‚Üí pick A‚ÄìB (1), then A‚ÄìC (2), then C‚ÄìD (3)</p>
<p>Same MST as above</p>

<h1>üü¢ Kruskal‚Äôs Algorithm Overview</h1>
<p><strong>Goal:</strong> Find the Minimum Spanning Tree (MST) of a connected, weighted graph.</p>

<h2>Steps:</h2>
<ol>
<li>Sort all edges by weight in ascending order.</li>
<li>Initialize MST as empty.</li>
<li>Pick the smallest edge.</li>
<li>If it does not form a cycle with edges in MST, include it; otherwise discard.</li>
<li>Repeat step 3 until MST has V-1 edges (V = number of vertices).</li>
</ol>

<p><strong>Cycle Detection:</strong> Use Union-Find / Disjoint Set data structure.</p>

<h2>Properties</h2>
<ul>
<li>Works for connected graphs.</li>
<li>Can handle edges with equal weights.</li>
<li>Time Complexity: O(E log E) (for sorting edges)</li>
</ul>

<h2>C Code Example</h2>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

typedef struct {
    int u, v, weight;
} Edge;

int compare(const void *a, const void *b) {
    return ((Edge*)a)-&gt;weight - ((Edge*)b)-&gt;weight;
}

int find(int parent[], int i) {
    if (parent[i] == i) return i;
    return parent[i] = find(parent, parent[i]); // Path compression
}

void unionSet(int parent[], int rank[], int x, int y) {
    int xroot = find(parent, x);
    int yroot = find(parent, y);
    if (rank[xroot] &lt; rank[yroot])
        parent[xroot] = yroot;
    else if (rank[xroot] &gt; rank[yroot])
        parent[yroot] = xroot;
    else {
        parent[yroot] = xroot;
        rank[xroot]++;
    }
}

void kruskal(Edge edges[], int V, int E) {
    qsort(edges, E, sizeof(Edge), compare);

    int parent[V], rank[V];
    for (int i = 0; i &lt; V; i++) { parent[i] = i; rank[i] = 0; }

    printf("Edges in MST:\\n");
    int count = 0;
    for (int i = 0; i &lt; E &amp;&amp; count &lt; V-1; i++) {
        int u = edges[i].u;
        int v = edges[i].v;
        int set_u = find(parent, u);
        int set_v = find(parent, v);
        if (set_u != set_v) {
            printf("%d -- %d == %d\\n", u, v, edges[i].weight);
            unionSet(parent, rank, set_u, set_v);
            count++;
        }
    }
}

int main() {
    int V = 4; 
    int E = 5;
    Edge edges[] = {
        {0, 1, 10},
        {0, 2, 6},
        {0, 3, 5},
        {1, 3, 15},
        {2, 3, 4}
    };
    kruskal(edges, V, E);
    return 0;
}
</code></pre>

<h2>Example Output</h2>
<pre><code>Edges in MST:
2 -- 3 == 4
0 -- 3 == 5
0 -- 1 == 10

Total MST weight = 4 + 5 + 10 = 19
</code></pre>
<h1>üå≥ Prim's Algorithm</h1>

<h2>1. Goal</h2>
<p>Find the Minimum Spanning Tree (MST) of a connected, weighted graph.</p>

<h2>2. Idea</h2>
<ul>
    <li>Start with any vertex.</li>
    <li>Grow the MST one edge at a time by picking the minimum weight edge that connects a vertex in the MST to a vertex outside it.</li>
    <li>Repeat until all vertices are included.</li>
</ul>

<h2>3. Steps</h2>
<ol>
    <li>Initialize a <code>key[]</code> array to track the minimum weight to reach each vertex.</li>
    <li>Initialize a <code>visited[]</code> (or MST set) array to track which vertices are included in MST.</li>
    <li>Start from any vertex (e.g., vertex 0), set <code>key[0] = 0</code>.</li>
    <li>Repeat for V vertices:
        <ul>
            <li>Pick the vertex <code>u</code> not in MST with minimum key</li>
            <li>Include <code>u</code> in MST</li>
            <li>For each neighbor <code>v</code> of <code>u</code>, if <code>v</code> not in MST and <code>weight(u,v) < key[v]</code>, update <code>key[v] = weight(u,v)</code></li>
        </ul>
    </li>
    <li>Print the MST edges and total weight.</li>
</ol>

<h2>4. C Code Implementation</h2>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;limits.h&gt;

#define V 5  // Number of vertices

int minKey(int key[], int mstSet[]) {
    int min = INT_MAX, min_index;
    for (int v = 0; v &lt; V; v++)
        if (mstSet[v] == 0 &amp;&amp; key[v] &lt; min)
            min = key[v], min_index = v;
    return min_index;
}

void primMST(int graph[V][V]) {
    int parent[V];
    int key[V];
    int mstSet[V];

    for (int i = 0; i &lt; V; i++)
        key[i] = INT_MAX, mstSet[i] = 0;

    key[0] = 0;
    parent[0] = -1;

    for (int count = 0; count &lt; V-1; count++) {
        int u = minKey(key, mstSet);
        mstSet[u] = 1;

        for (int v = 0; v &lt; V; v++)
            if (graph[u][v] &amp;&amp; mstSet[v] == 0 &amp;&amp; graph[u][v] &lt; key[v])
                parent[v] = u, key[v] = graph[u][v];
    }

    printf("Edge \tWeight\n");
    int totalWeight = 0;
    for (int i = 1; i &lt; V; i++) {
        printf("%d - %d \t%d\n", parent[i], i, graph[i][parent[i]]);
        totalWeight += graph[i][parent[i]];
    }
    printf("Total Weight of MST: %d\n", totalWeight);
}

int main() {
    int graph[V][V] = {
        {0, 2, 0, 6, 0},
        {2, 0, 3, 8, 5},
        {0, 3, 0, 0, 7},
        {6, 8, 0, 0, 9},
        {0, 5, 7, 9, 0}
    };
    primMST(graph);
    return 0;
}
</code></pre>

<h2>5. Sample Output</h2>
<table>
<tr>
<th>Edge</th>
<th>Weight</th>
</tr>
<tr>
<td>0 - 1</td>
<td>2</td>
</tr>
<tr>
<td>1 - 2</td>
<td>3</td>
</tr>
<tr>
<td>0 - 3</td>
<td>6</td>
</tr>
<tr>
<td>1 - 4</td>
<td>5</td>
</tr>
<tr>
<th>Total Weight of MST</th>
<th>16</th>
</tr>
</table>

<h2>6. ‚úÖ Explanation of Code</h2>
<ul>
    <li><code>minKey()</code> finds the vertex with the smallest key not yet in MST.</li>
    <li><code>key[]</code> stores the minimum weight edge for each vertex.</li>
    <li><code>parent[]</code> stores the MST structure.</li>
    <li>We iterate V-1 times because an MST has V-1 edges.</li>
</ul>

<h1>üåê Strongly Connected Components (SCC) in Graphs</h1>

<h2>1. Definition</h2>
<p>A <strong>Strongly Connected Component (SCC)</strong> in a directed graph is a maximal set of vertices such that:</p>
<ul>
<li>For every pair of vertices <code>u</code> and <code>v</code> in the SCC, there is a path from <code>u ‚Üí v</code> and a path from <code>v ‚Üí u</code>.</li>
<li>In simpler words, all nodes in an SCC can reach each other.</li>
</ul>

<h2>2. Why SCCs Matter</h2>
<ul>
<li>Useful in network analysis, dependency analysis, compilers (detecting cycles), and social networks.</li>
<li>Condenses a graph into components forming a Directed Acyclic Graph (DAG).</li>
</ul>

<h2>3. Algorithms to Find SCC</h2>

<h3>3.1 Kosaraju‚Äôs Algorithm</h3>
<ol>
<li>Perform a DFS of the original graph, maintaining a stack of finished vertices.</li>
<li>Transpose the graph (reverse all edges).</li>
<li>Pop vertices from the stack and perform DFS on the transposed graph.</li>
<li>Each DFS traversal produces a strongly connected component.</li>
</ol>
<p><strong>Time Complexity:</strong> O(V + E) &nbsp; | &nbsp; <strong>Space Complexity:</strong> O(V + E)</p>

<h3>3.2 Tarjan‚Äôs Algorithm</h3>
<ul>
<li>DFS-based approach using low-link values to identify SCCs in a single DFS traversal.</li>
<li>Stores nodes in a stack and marks components when backtracking.</li>
<li><strong>Time Complexity:</strong> O(V + E) &nbsp; | &nbsp; <strong>Space Complexity:</strong> O(V)</li>
</ul>

<h2>4. Example Directed Graph</h2>
<p><strong>Vertices:</strong> 5</p>
<p><strong>Edges:</strong></p>
<ul>
<li>0 ‚Üí 2, 0 ‚Üí 3</li>
<li>1 ‚Üí 0</li>
<li>2 ‚Üí 1</li>
<li>3 ‚Üí 4</li>
<li>4 ‚Üí 3</li>
</ul>

<p><strong>SCCs:</strong></p>
<ul>
<li>{0, 1, 2}</li>
<li>{3, 4}</li>
</ul>

<h2>5. C Implementation (Kosaraju‚Äôs Algorithm)</h2>
<pre><code>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;

#define MAX 100

typedef struct Node {
    int vertex;
    struct Node* next;
} Node;

Node* createNode(int v) {
    Node* newNode = (Node*)malloc(sizeof(Node));
    newNode-&gt;vertex = v;
    newNode-&gt;next = NULL;
    return newNode;
}

void addEdge(Node* adj[], int u, int v) {
    Node* newNode = createNode(v);
    newNode-&gt;next = adj[u];
    adj[u] = newNode;
}

// Stack for DFS finish times
int stack[MAX], top = -1;
void push(int v) { stack[++top] = v; }
int pop() { return stack[top--]; }

// DFS for first pass
void DFS1(Node* adj[], int v, int visited[]) {
    visited[v] = 1;
    Node* temp = adj[v];
    while (temp) {
        if (!visited[temp-&gt;vertex])
            DFS1(adj, temp-&gt;vertex, visited);
        temp = temp-&gt;next;
    }
    push(v);
}

// DFS for second pass (on transposed graph)
void DFS2(Node* adjT[], int v, int visited[]) {
    visited[v] = 1;
    printf("%d ", v);
    Node* temp = adjT[v];
    while (temp) {
        if (!visited[temp-&gt;vertex])
            DFS2(adjT, temp-&gt;vertex, visited);
        temp = temp-&gt;next;
    }
}

int main() {
    int V = 5;
    Node* adj[V];
    Node* adjT[V];
    for (int i = 0; i &lt; V; i++) adj[i] = adjT[i] = NULL;

    // Add edges
    addEdge(adj, 0, 2);
    addEdge(adj, 0, 3);
    addEdge(adj, 1, 0);
    addEdge(adj, 2, 1);
    addEdge(adj, 3, 4);
    addEdge(adj, 4, 3);

    int visited[V];
    for (int i = 0; i &lt; V; i++) visited[i] = 0;

    // First pass DFS
    for (int i = 0; i &lt; V; i++)
        if (!visited[i]) DFS1(adj, i, visited);

    // Transpose graph
    for (int v = 0; v &lt; V; v++) {
        Node* temp = adj[v];
        while (temp) {
            addEdge(adjT, temp-&gt;vertex, v);
            temp = temp-&gt;next;
        }
    }

    // Second pass DFS
    for (int i = 0; i &lt; V; i++) visited[i] = 0;
    printf("Strongly Connected Components:\n");
    while (top != -1) {
        int v = pop();
        if (!visited[v]) {
            DFS2(adjT, v, visited);
            printf("\n");
        }
    }

    return 0;
}</code></pre>

<h2>6. Sample Output</h2>
<pre><code>Strongly Connected Components:
3 4
0 2 1
</code></pre>
<p>Each line represents an SCC.</p>

<h2>‚úÖ Key Points</h2>
<ul>
<li>SCCs exist only in directed graphs.</li>
<li>Kosaraju and Tarjan are efficient algorithms for SCC detection.</li>
<li>SCCs help reduce a graph to a DAG of components.</li>
</ul>
<h1>1. What is a Hash Table?</h1>
<p>A <strong>Hash Table</strong> (or Hash Map) is a data structure that stores <strong>key-value pairs</strong> and allows fast insertion, deletion, and lookup‚Äîon average in <strong>O(1)</strong> time.</p>
<p>Think of it like a dictionary: given a key, you quickly get its value.</p>

<h2>2. How Does It Work?</h2>
<h3>Hash Function</h3>
<p>Converts a key into an index in an array.</p>
<pre>Example: hash(key) = key % size_of_array</pre>
<p>This ensures that keys are distributed across the array.</p>

<h3>Array (Buckets)</h3>
<p>The hash function gives the <strong>index</strong> of the array where the value will be stored.</p>

<h3>Handling Collisions</h3>
<p>Sometimes, two keys may map to the same index. This is called a <strong>collision</strong>.</p>
<p>Techniques to handle collisions:</p>
<ul>
<li><strong>Chaining:</strong> Store multiple elements in a linked list at that index.</li>
<li><strong>Open Addressing:</strong> Find another empty slot in the array:
  <ul>
    <li>Linear Probing</li>
    <li>Quadratic Probing</li>
    <li>Double Hashing</li>
  </ul>
</li>
</ul>

<h2>3. Operations on a Hash Table</h2>
<table>
<tr>
<th>Operation</th>
<th>Description</th>
<th>Time Complexity (Average)</th>
</tr>
<tr><td>Insert</td><td>Add a key-value pair</td><td>O(1)</td></tr>
<tr><td>Search</td><td>Find value by key</td><td>O(1)</td></tr>
<tr><td>Delete</td><td>Remove key-value pair</td><td>O(1)</td></tr>
</table>
<p><em>Worst case: O(n) if all keys collide (rare if hash function is good).</em></p>

<h2>4. Example</h2>
<p>Suppose we want to store student marks:</p>
<table>
<tr><th>Key (Student ID)</th><th>Value (Marks)</th></tr>
<tr><td>101</td><td>95</td></tr>
<tr><td>102</td><td>88</td></tr>
<tr><td>203</td><td>76</td></tr>
</table>

<p>Hash function: <code>key % 10</code></p>
<ul>
<li>101 ‚Üí 1</li>
<li>102 ‚Üí 2</li>
<li>203 ‚Üí 3</li>
</ul>

<p>Array after insertion:</p>
<pre>
Index: 0 1 2 3 4 5 6 7 8 9
Value: - 101 102 203 - - - - - -
</pre>

<p>If we search for ID 102, hash ‚Üí index 2 ‚Üí value = 88.</p>

<h2>5. Advantages</h2>
<ul>
<li>Fast access: O(1) average</li>
<li>Efficient for lookup-heavy operations</li>
<li>Flexible: keys can be strings, numbers, or objects (with proper hash function)</li>
</ul>

<h2>6. Disadvantages</h2>
<ul>
<li>Can waste memory if table is too large</li>
<li>Performance depends on good hash function</li>
<li>Worst-case performance O(n) (many collisions)</li>
</ul>

<h2>7. Real-world Applications</h2>
<ul>
<li>Dictionaries / Maps (like Python‚Äôs <code>dict</code>)</li>
<li>Caching (like LRU cache)</li>
<li>Database indexing</li>
<li>Implementing sets</li>
</ul>
<h1>Hash Table Collision Resolution Techniques</h1>

<h2>1. Chaining (Separate Chaining)</h2>
<p><strong>Idea:</strong> Store multiple elements at the same index using a linked list (or dynamic array).</p>

<h3>Steps:</h3>
<ol>
    <li>Compute hash index: <code>index = hash(key) % table_size</code></li>
    <li>If slot is empty ‚Üí insert</li>
    <li>If slot is occupied ‚Üí append to the linked list at that index</li>
</ol>

<h3>Example:</h3>
<pre>
Hash Table Size = 5
Keys: 7, 12, 17
Hash Function: key % 5

7 % 5 = 2 ‚Üí insert 7 at index 2
12 % 5 = 2 ‚Üí insert 12 at index 2 (collision)
Linked list: 7 ‚Üí 12
17 % 5 = 2 ‚Üí insert 17
Linked list: 7 ‚Üí 12 ‚Üí 17
</pre>

<h3>Advantages:</h3>
<ul>
    <li>Simple to implement</li>
    <li>Table can handle more elements than its size</li>
</ul>

<h3>Disadvantages:</h3>
<ul>
    <li>Extra memory for linked lists</li>
    <li>If many collisions ‚Üí long lists ‚Üí slower search (O(n) in worst case)</li>
</ul>

<hr>

<h2>2. Open Addressing</h2>
<p><strong>Idea:</strong> Store all elements directly in the hash table. If a slot is occupied, find the next available slot using a probe sequence.</p>

<h3>Probing Techniques:</h3>
<ul>
    <li><strong>Linear Probing:</strong> <code>index = (hash(key) + i) % table_size</code>  
        Check next slot sequentially until empty. Problem: clustering.</li>
    <li><strong>Quadratic Probing:</strong> <code>index = (hash(key) + i¬≤) % table_size</code>  
        Reduces clustering.</li>
    <li><strong>Double Hashing:</strong> <code>index = (hash1(key) + i * hash2(key)) % table_size</code>  
        Uses a second hash function to compute step size.</li>
</ul>

<h3>Example (Linear Probing):</h3>
<pre>
Table size = 5
Keys: 7, 12, 17
Hash: key % 5

7 ‚Üí index 2 ‚Üí insert
12 ‚Üí index 2 occupied ‚Üí check index 3 ‚Üí insert
17 ‚Üí index 2 occupied ‚Üí check index 3 ‚Üí occupied ‚Üí index 4 ‚Üí insert
</pre>

<h3>Advantages:</h3>
<ul>
    <li>No extra memory for lists</li>
    <li>Good cache performance</li>
</ul>

<h3>Disadvantages:</h3>
<ul>
    <li>Table must not be too full (load factor &lt; 0.7)</li>
    <li>Clustering slows down operations</li>
</ul>

<h2>Summary Table</h2>
<table>
<tr>
<th>Feature</th>
<th>Chaining</th>
<th>Open Addressing</th>
</tr>
<tr><td>Extra memory needed?</td><td>Yes (for linked lists)</td><td>No</td></tr>
<tr><td>Max elements</td><td>Unlimited (depends on memory)</td><td>&lt;= table size</td></tr>
<tr><td>Performance on collisions</td><td>Depends on list length</td><td>Depends on load factor</td></tr>
<tr><td>Cache friendly</td><td>No</td><td>Yes</td></tr>
</table>

<h1>What is an Algorithm?</h1>
<p>An <strong>algorithm</strong> is a step-by-step procedure to solve a problem or perform a task.</p>

<h2>Key Properties of an Algorithm</h2>
<ul>
    <li><strong>Input ‚Üí Process ‚Üí Output:</strong> Algorithms take input, perform a series of steps, and produce output.</li>
    <li><strong>Must terminate:</strong> An algorithm must finish after a finite number of steps.</li>
    <li><strong>Must be correct and efficient:</strong> It should solve the problem correctly using minimal resources (time and space).</li>
</ul>
<h1>Brute Force Algorithm</h1>

<h2>1. Definition</h2>
<p>A Brute Force algorithm is the simplest approach to solve a problem by trying all possible solutions and selecting the correct one. It does not use any shortcuts or optimization. Very easy to implement but often inefficient for large input sizes.</p>

<h2>2. Characteristics</h2>
<ul>
<li>Simple and straightforward.</li>
<li>Guarantees a correct solution (if one exists).</li>
<li>Usually high time complexity.</li>
<li>Works well for small datasets.</li>
</ul>

<h2>3. Steps to Solve a Problem Using Brute Force</h2>
<ol>
<li>List all possible solutions.</li>
<li>Check each solution to see if it satisfies the problem‚Äôs conditions.</li>
<li>Return the solution(s) that satisfy the conditions.</li>
</ol>

<h2>4. Examples</h2>

<h3>a) Linear Search</h3>
<p><strong>Problem:</strong> Find a key in an array.</p>
<p><strong>Algorithm:</strong> Check each element one by one until you find the key.</p>
<p><strong>Complexity:</strong></p>
<ul>
<li>Best case: O(1) ‚Üí first element is the key</li>
<li>Worst case: O(n) ‚Üí last element or not found</li>
</ul>

<h3>b) Maximum Subarray Sum (Naive)</h3>
<p><strong>Problem:</strong> Find the subarray with the maximum sum.</p>
<p><strong>Algorithm:</strong> Check the sum of all possible subarrays and pick the largest.</p>
<p><strong>Complexity:</strong> O(n¬≥) (or O(n¬≤) with slight optimization)</p>

<h3>c) String Matching (Naive)</h3>
<p><strong>Problem:</strong> Find a substring in a string.</p>
<p><strong>Algorithm:</strong> Compare the substring with all possible positions in the main string.</p>
<p><strong>Complexity:</strong> O((n-m+1) * m) ‚Üí n = text length, m = pattern length</p>

<h3>d) Traveling Salesman Problem (Naive)</h3>
<p><strong>Problem:</strong> Find the shortest route visiting all cities.</p>
<p><strong>Algorithm:</strong> Check all possible permutations of cities and select the shortest path.</p>
<p><strong>Complexity:</strong> O(n!) ‚Üí factorial, very slow for large n</p>

<h2>5. Advantages</h2>
<ul>
<li>Very simple to implement.</li>
<li>Works for any problem size if small.</li>
<li>Always finds the correct solution.</li>
</ul>

<h2>6. Disadvantages</h2>
<ul>
<li>Extremely inefficient for large inputs.</li>
<li>Time complexity grows very fast with input size.</li>
<li>Not suitable for real-time or large-scale problems.</li>
</ul>

<h2>7. When to Use Brute Force</h2>
<ul>
<li>Input size is small.</li>
<li>Problem is simple and does not require optimization.</li>
<li>As a baseline to test more efficient algorithms.</li>
</ul>
<h1>2. Divide and Conquer</h1>

<h2>Definition</h2>
<p>Divide and Conquer is an algorithmic paradigm where a problem is divided into smaller subproblems, each solved recursively, and then the results are combined to form the final solution.</p>

<h2>Idea</h2>
<ul>
    <li>Break a big problem into smaller, manageable subproblems.</li>
    <li>Solve the subproblems individually.</li>
    <li>Combine solutions efficiently to solve the original problem.</li>
    <li>Works best for problems with recursive structure.</li>
</ul>

<h2>Steps</h2>
<ul>
    <li><strong>Divide:</strong> Split the problem into smaller subproblems.<br>
        <em>Example:</em> Divide an array into two halves.</li>
    <li><strong>Conquer:</strong> Solve each subproblem recursively.<br>
        <em>Example:</em> Sort the left half and right half.</li>
    <li><strong>Combine:</strong> Merge the solutions of the subproblems.<br>
        <em>Example:</em> Merge two sorted halves into a sorted array.</li>
</ul>

<h2>Examples</h2>
<table>
<tr>
<th>Algorithm</th>
<th>Description</th>
<th>Complexity</th>
</tr>
<tr><td>Merge Sort</td><td>Divide array into halves, sort recursively, merge sorted halves</td><td>O(n log n)</td></tr>
<tr><td>Quick Sort</td><td>Pick a pivot, partition array, sort partitions recursively</td><td>O(n log n) average, O(n¬≤) worst</td></tr>
<tr><td>Binary Search</td><td>Divide search space into halves to find target</td><td>O(log n)</td></tr>
<tr><td>Strassen‚Äôs Matrix Multiplication</td><td>Divide matrices into submatrices and multiply recursively</td><td>O(n^2.81)</td></tr>
</table>

<h2>Advantages</h2>
<ul>
    <li>Reduces problem size at each step ‚Üí faster than brute force.</li>
    <li>Efficient for recursive problems like sorting, searching, and matrix operations.</li>
    <li>Often leads to simpler code for complex problems.</li>
</ul>

<h2>Complexity</h2>
<p>Typical complexity: O(n log n) (e.g., Merge Sort, Binary Search)</p>
<p>Depends on number of subproblems and cost to combine results.</p>

<h3>Recurrence Relation</h3>
<p>For a problem of size <em>n</em>:</p>
<pre>
T(n) = a * T(n / b) + f(n)
</pre>
<ul>
    <li><strong>a</strong> = number of subproblems</li>
    <li><strong>n/b</strong> = size of each subproblem</li>
    <li><strong>f(n)</strong> = cost to combine results</li>
</ul>

<p><em>Example:</em> Merge Sort ‚Üí T(n) = 2T(n/2) + O(n) ‚Üí O(n log n)</p>
<h1>3. Greedy Algorithms</h1>

<h2>Definition</h2>
<p>A greedy algorithm always makes the best choice at the current step, without worrying about future consequences, hoping this will lead to the global optimum.</p>

<h2>Key Idea</h2>
<p>Greedy algorithms work when a problem has:</p>
<ul>
<li><strong>Greedy Choice Property:</strong> Locally optimal choice leads to globally optimal solution.</li>
<li><strong>Optimal Substructure:</strong> Solution of subproblems can be combined to form a solution for the overall problem.</li>
</ul>

<h2>Steps (How It Works)</h2>
<ol>
<li>Choose the best local option at the current step.</li>
<li>Add it to the current solution.</li>
<li>Repeat until a complete solution is formed.</li>
</ol>

<h2>Examples</h2>

<h3>Activity Selection Problem</h3>
<ul>
<li><strong>Problem:</strong> Schedule the maximum number of non-overlapping activities.</li>
<li><strong>Greedy Choice:</strong> Always pick the activity that ends earliest.</li>
<li><strong>Complexity:</strong> O(n log n) (due to sorting by finish time)</li>
</ul>

<h3>Huffman Encoding</h3>
<ul>
<li><strong>Problem:</strong> Generate minimum-length prefix code for characters based on frequency.</li>
<li><strong>Greedy Choice:</strong> Always combine two nodes with smallest frequency.</li>
<li><strong>Complexity:</strong> O(n log n)</li>
</ul>

<h3>Minimum Spanning Tree (MST)</h3>
<ul>
<li><strong>Prim‚Äôs Algorithm:</strong> Always add the smallest edge connecting a new vertex.</li>
<li><strong>Kruskal‚Äôs Algorithm:</strong> Always pick the next smallest edge that doesn‚Äôt form a cycle.</li>
<li><strong>Complexity:</strong> O(E log V) (depends on edge sorting or priority queue operations)</li>
</ul>

<h2>Advantages</h2>
<ul>
<li>Simple and fast</li>
<li>Efficient for problems with greedy property</li>
</ul>

<h2>Disadvantages</h2>
<ul>
<li>Doesn‚Äôt work for all problems</li>
<li>Can fail if local optimum does not lead to global optimum</li>
</ul>

<h2>Complexity</h2>
<p>Sorting or priority queue operations dominate: Usually O(n log n). Some greedy problems can be solved in O(n) if no sorting is needed.</p>

<h2>Intuition</h2>
<p>Think of greedy algorithms like choosing coins to make change:</p>
<ul>
<li>Always pick the largest coin available until the amount is complete.</li>
<li>Works for standard coin systems (like USD) but may fail for arbitrary coin systems.</li>
</ul>
<h1>üìå Dynamic Programming (DP)</h1>

<h2>Definition</h2>
<p>Dynamic Programming is a technique to solve problems by breaking them into smaller <strong>overlapping subproblems</strong> and storing their results to avoid recomputation.</p>
<ul>
    <li><strong>Overlapping Subproblems:</strong> Same subproblem is solved multiple times.</li>
    <li><strong>Optimal Substructure:</strong> Solution of the problem can be constructed from solutions of subproblems.</li>
</ul>

<h2>Approaches</h2>
<h3>1. Memoization (Top-Down)</h3>
<ul>
    <li>Solve recursively.</li>
    <li>Store results in a table (array/hash map) to avoid recomputation.</li>
    <li>Example: Fibonacci numbers using recursion + memo table.</li>
</ul>

<h3>2. Tabulation (Bottom-Up)</h3>
<ul>
    <li>Solve iteratively starting from the smallest subproblem.</li>
    <li>Build up a table until the final solution is reached.</li>
    <li>Often more space-efficient than memoization.</li>
</ul>

<h2>Steps to Solve DP Problems</h2>
<ol>
    <li><strong>Identify the subproblems:</strong> Break the problem into smaller pieces.</li>
    <li><strong>Formulate recurrence relation:</strong> Express solution in terms of subproblems.<br>
        Example (Fibonacci): <code>F(n) = F(n-1) + F(n-2)</code>
    </li>
    <li><strong>Compute and store subproblem results:</strong> Use array or hash table.</li>
    <li><strong>Build the final solution:</strong> Combine stored results to get the answer for the original problem.</li>
</ol>

<h2>Examples</h2>

<h3>1. Fibonacci Sequence</h3>
<pre>
F(0) = 0, F(1) = 1
F(n) = F(n-1) + F(n-2)

DP computes each Fibonacci number once and stores it.
</pre>

<h3>2. 0/1 Knapsack Problem</h3>
<p>Maximize value without exceeding weight.</p>
<pre>
Recurrence:
dp[i][w] = max(dp[i-1][w], value[i] + dp[i-1][w-weight[i]])

Tabulation fills a 2D table of size n x W.
</pre>

<h3>3. Longest Common Subsequence (LCS)</h3>
<p>Find the longest sequence present in both strings.</p>
<pre>
Recurrence:
if X[i] == Y[j]:
    dp[i][j] = 1 + dp[i-1][j-1]
else:
    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
</pre>

<h3>4. Matrix Chain Multiplication</h3>
<p>Find minimum cost of multiplying a chain of matrices.</p>
<pre>
Recurrence:
dp[i][j] = min(dp[i][k] + dp[k+1][j] + cost of multiplying Ai..Ak & Ak+1..Aj)
</pre>

<h2>Advantages</h2>
<ul>
    <li>Efficient for problems with overlapping subproblems.</li>
    <li>Avoids repeated computation.</li>
    <li>Often converts exponential recursive solutions into polynomial time.</li>
</ul>

<h2>Complexity</h2>
<ul>
    <li><strong>Time:</strong> O(n¬≤) for most table-based DP problems like LCS or Matrix Chain Multiplication.</li>
    <li><strong>Space:</strong> O(n¬≤) usually; can be optimized to O(n) using rolling arrays.</li>
</ul>
<h1>üîÑ Backtracking Algorithm</h1>

<h2>Definition</h2>
<p>
Backtracking is a systematic way of trying out all possible solutions to a problem, abandoning a solution (‚Äúbacktracking‚Äù) as soon as it is determined that it cannot lead to a valid complete solution.
</p>

<h2>Idea</h2>
<ul>
<li>Think of it as <strong>Depth-First Search (DFS)</strong> on the solution space.</li>
<li>If a partial solution is invalid, it prunes that path immediately.</li>
<li>Efficiently explores large solution spaces without checking all possibilities blindly.</li>
</ul>

<h2>Steps</h2>
<ol>
<li>Choose a candidate for the next step in the solution.</li>
<li>Check feasibility:
    <ul>
        <li>If valid ‚Üí proceed recursively.</li>
        <li>If invalid ‚Üí backtrack to previous step.</li>
    </ul>
</li>
<li>Repeat until all possibilities are explored or a solution is found.</li>
</ol>

<h2>Examples</h2>
<ul>
<li><strong>N-Queens Problem:</strong> Place N queens on an N√óN chessboard so that no two queens attack each other. Place a queen row by row ‚Üí backtrack if a conflict occurs.</li>
<li><strong>Sudoku Solver:</strong> Fill empty cells ‚Üí check constraints ‚Üí backtrack on invalid placements.</li>
<li><strong>Maze Solving:</strong> Move through paths recursively ‚Üí backtrack if hitting walls or dead ends.</li>
</ul>

<h2>Complexity</h2>
<table>
<tr><th>Type</th><th>Complexity</th></tr>
<tr><td>Time</td><td>O(branch^depth) ‚Äì usually exponential</td></tr>
<tr><td>Space</td><td>O(depth) ‚Äì recursion stack</td></tr>
</table>

<h2>Simple Example: N-Queens (4x4)</h2>
<pre>
Place queen in row 0 ‚Üí check conflicts
    If valid ‚Üí move to row 1
        If row 1 placement invalid ‚Üí backtrack to row 0
Continue recursively until all rows are filled
</pre>
<p>
Using backtracking, not all 4^4 = 256 placements are checked; invalid paths are abandoned early.
</p>
<h1>üåê Branch and Bound (B&amp;B)</h1>

<h2>Definition</h2>
<p>Branch and Bound is an optimization algorithm that systematically explores the solution space of a problem. It <strong>branches</strong> into subproblems and <strong>bounds/prunes</strong> subproblems that cannot yield better solutions than the best one found so far.</p>

<h2>Key Idea</h2>
<p>Instead of exploring all possible solutions (like brute force), B&amp;B eliminates large parts of the search space that cannot lead to an optimal solution.</p>
<p>Uses a tree structure:</p>
<ul>
<li>Each node = partial solution</li>
<li>Leaf nodes = complete solutions</li>
</ul>

<h2>Steps</h2>
<ol>
<li><strong>Represent solution space as a tree:</strong>
    <ul>
        <li>Root = no decisions made</li>
        <li>Children = decisions made at each step</li>
    </ul>
</li>
<li><strong>Explore promising branches:</strong> Usually depth-first or best-first search.</li>
<li><strong>Bound / Prune branches:</strong> If a partial solution cannot beat the best known solution ‚Üí discard it.</li>
<li><strong>Update best solution:</strong> When a complete solution better than current best is found, update it.</li>
</ol>

<h2>Example: Travelling Salesman Problem (TSP)</h2>
<p><strong>Problem:</strong> Find the shortest route visiting all cities exactly once and returning to start.</p>
<ul>
<li><strong>Branch:</strong> Choose the next city to visit.</li>
<li><strong>Bound:</strong> Calculate lower bound of tour length from partial route.</li>
<li><strong>Prune:</strong> If lower bound ‚â• current best ‚Üí discard branch.</li>
<li><strong>Result:</strong> Only explores paths that can potentially beat current best route.</li>
</ul>

<h2>Advantages</h2>
<ul>
<li>Reduces search space significantly compared to brute force</li>
<li>Guarantees optimal solution</li>
<li>Flexible; works for many combinatorial optimization problems</li>
</ul>

<h2>Disadvantages</h2>
<ul>
<li>Still exponential time in worst case</li>
<li>Memory intensive (storing tree nodes or active branches)</li>
<li>Performance highly depends on the quality of bounding function</li>
</ul>

<h2>Other Examples</h2>
<ul>
<li>Integer Linear Programming (ILP)</li>
<li>Knapsack Problem</li>
<li>Job Scheduling Problems</li>
</ul>

<h1>Algorithm Paradigms</h1>

<table>
<thead>
<tr>
<th>Paradigm</th>
<th>Idea</th>
<th>Examples</th>
<th>Complexity</th>
<th>When to Use</th>
</tr>
</thead>
<tbody>
<tr>
<td>Brute Force</td>
<td>Try all possibilities</td>
<td>Linear Search, Subset Sum</td>
<td>O(n¬≤) or worse</td>
<td>Small input sizes or simple problems</td>
</tr>
<tr>
<td>Divide & Conquer</td>
<td>Divide, Solve, Combine</td>
<td>Merge Sort, Binary Search</td>
<td>O(n log n)</td>
<td>Problems that can be split into independent subproblems</td>
</tr>
<tr>
<td>Greedy</td>
<td>Locally optimal choices</td>
<td>Activity Selection, MST</td>
<td>O(n log n)</td>
<td>Problems with Greedy Choice Property + Optimal Substructure</td>
</tr>
<tr>
<td>Dynamic Programming</td>
<td>Solve overlapping subproblems</td>
<td>Fibonacci, Knapsack, LCS</td>
<td>O(n¬≤)</td>
<td>Problems with overlapping subproblems and optimal substructure</td>
</tr>
<tr>
<td>Backtracking</td>
<td>DFS + discard invalid</td>
<td>N-Queens, Sudoku</td>
<td>Exponential</td>
<td>Constraint satisfaction problems, combinatorial problems</td>
</tr>
<tr>
<td>Branch & Bound</td>
<td>Prune non-promising branches</td>
<td>TSP, ILP</td>
<td>Problem dependent</td>
<td>Optimization problems where pruning can reduce search space</td>
</tr>
</tbody>
</table>
<h1>üîç Search Algorithms</h1>

<h2>1. Linear Search (Sequential Search)</h2>
<p><strong>Definition:</strong> Check each element one by one until the target is found or the array ends.</p>
<p><strong>Properties:</strong> Works on unsorted and sorted arrays.</p>
<p><strong>Time Complexity:</strong></p>
<ul>
<li>Best Case: O(1) ‚Üí target is first element</li>
<li>Worst Case: O(n) ‚Üí target is last or not present</li>
</ul>
<p><strong>Space Complexity:</strong> O(1) ‚Üí no extra space needed</p>

<h3>Algorithm (Pseudocode)</h3>
<pre><code>function linearSearch(arr, key):
    for i = 0 to n-1:
        if arr[i] == key:
            return i   // index found
    return -1         // not found
</code></pre>

<h3>Example</h3>
<p>Array: [4, 2, 7, 1, 9]<br>
Search for 7</p>
<ul>
<li>Step 1: Check 4 ‚Üí No</li>
<li>Step 2: Check 2 ‚Üí No</li>
<li>Step 3: Check 7 ‚Üí Yes ‚Üí Found at index 2</li>
</ul>

<p><strong>Use Cases:</strong> Small datasets, unsorted data.</p>

<hr>

<h2>2. Binary Search</h2>
<p><strong>Definition:</strong> Efficient search for a sorted array using divide and conquer. Repeatedly divide array in half and compare the middle element with target.</p>
<p><strong>Properties:</strong> Works only on sorted arrays.</p>
<p><strong>Time Complexity:</strong> O(log n) ‚Üí each step reduces search space by half.</p>
<p><strong>Space Complexity:</strong></p>
<ul>
<li>Iterative: O(1)</li>
<li>Recursive: O(log n) (due to recursion stack)</li>
</ul>

<h3>Algorithm (Pseudocode, Iterative)</h3>
<pre><code>function binarySearch(arr, key):
    low = 0
    high = n - 1
    while low <= high:
        mid = (low + high) / 2
        if arr[mid] == key:
            return mid
        else if arr[mid] < key:
            low = mid + 1
        else:
            high = mid - 1
    return -1
</code></pre>

<h3>Example</h3>
<p>Sorted Array: [1, 3, 5, 7, 9, 11]<br>
Search for 7</p>
<ul>
<li>Step 1: mid = 5 (index 2) ‚Üí 5 < 7 ‚Üí search right half</li>
<li>Step 2: mid = 7 (index 3) ‚Üí Found</li>
</ul>

<p><strong>Variants:</strong> Recursive Binary Search, Binary Search on rotated arrays, Lower bound / upper bound search.</p>
<p><strong>Use Cases:</strong> Large sorted datasets, searching in databases, dictionaries, or indexes.</p>

<h2>Comparison Table</h2>
<table>
<tr>
<th>Feature</th>
<th>Linear Search</th>
<th>Binary Search</th>
</tr>
<tr><td>Array Sorted?</td><td>Not needed</td><td>Must be sorted</td></tr>
<tr><td>Time Complexity</td><td>O(n)</td><td>O(log n)</td></tr>
<tr><td>Space Complexity</td><td>O(1)</td><td>O(1) (iterative)</td></tr>
<tr><td>Approach</td><td>Sequential scan</td><td>Divide &amp; conquer</td></tr>
<tr><td>Best For</td><td>Small or unsorted arrays</td><td>Large sorted arrays</td></tr>
</table>
<h1>Sorting algorithm</h1>

<h1>Bubble Sort</h1>

<h2>Definition</h2>
<p>Bubble Sort is a simple comparison-based sorting algorithm where adjacent elements are repeatedly compared and swapped if they are in the wrong order.</p>
<p>Think of it like bubbles rising to the top: the largest element ‚Äúbubbles up‚Äù to its correct position after each pass.</p>

<h2>Algorithm Steps</h2>
<ol>
<li>Start from the first element of the array.</li>
<li>Compare the current element with the next element.</li>
<li>If the current element &gt; next element, swap them.</li>
<li>Move to the next pair and repeat.</li>
<li>Continue until the end of the array ‚Üí largest element is at the end.</li>
<li>Repeat the process for the remaining unsorted part.</li>
<li>Stop when no swaps are needed in a pass.</li>
</ol>

<h2>Pseudocode</h2>
<pre>
for i = 0 to n-1:
    swapped = false
    for j = 0 to n-i-2:
        if arr[j] &gt; arr[j+1]:
            swap(arr[j], arr[j+1])
            swapped = true
    if not swapped:
        break   // array is already sorted
</pre>

<h2>Example</h2>
<p>Array: [5, 2, 4, 1, 3]</p>

<h3>Pass 1:</h3>
<ul>
<li>Compare 5 &amp; 2 ‚Üí swap ‚Üí [2,5,4,1,3]</li>
<li>Compare 5 &amp; 4 ‚Üí swap ‚Üí [2,4,5,1,3]</li>
<li>Compare 5 &amp; 1 ‚Üí swap ‚Üí [2,4,1,5,3]</li>
<li>Compare 5 &amp; 3 ‚Üí swap ‚Üí [2,4,1,3,5]</li>
</ul>

<h3>Pass 2:</h3>
<ul>
<li>Compare 2 &amp; 4 ‚Üí no swap</li>
<li>Compare 4 &amp; 1 ‚Üí swap ‚Üí [2,1,4,3,5]</li>
<li>Compare 4 &amp; 3 ‚Üí swap ‚Üí [2,1,3,4,5]</li>
<li>Compare 4 &amp; 5 ‚Üí no swap</li>
</ul>

<h3>Pass 3:</h3>
<ul>
<li>Compare 2 &amp; 1 ‚Üí swap ‚Üí [1,2,3,4,5]</li>
<li>Compare 2 &amp; 3 ‚Üí no swap</li>
<li>Compare 3 &amp; 4 ‚Üí no swap</li>
</ul>

<h3>Pass 4:</h3>
<p>No swaps ‚Üí array is sorted</p>

<p><strong>Sorted Array:</strong> [1,2,3,4,5]</p>

<h2>Complexity</h2>
<table>
<tr><th>Case</th><th>Time Complexity</th><th>Notes</th></tr>
<tr><td>Best Case</td><td>O(n)</td><td>Already sorted (optimized with swap flag)</td></tr>
<tr><td>Worst Case</td><td>O(n¬≤)</td><td>Reverse sorted</td></tr>
<tr><td>Average Case</td><td>O(n¬≤)</td><td>Random order</td></tr>
<tr><td>Space</td><td>O(1)</td><td>In-place sorting</td></tr>
</table>

<h2>Properties</h2>
<ul>
<li>Stable: Yes (doesn‚Äôt change relative order of equal elements)</li>
<li>In-place: Yes</li>
<li>Adaptive: Yes (if no swaps in a pass ‚Üí stop)</li>
</ul>
<h1>Insertion Sort</h1>

<h2>Idea</h2>
<p>Insertion Sort builds a sorted portion of the array one element at a time. Each new element is inserted into its correct position in the sorted portion. Think of it like sorting playing cards in your hand.</p>

<h2>Algorithm (Step-by-Step)</h2>
<p>Given array <code>arr[0..n-1]</code>:</p>
<ol>
    <li>Start from the second element <code>arr[1]</code> (first element is considered sorted).</li>
    <li>Pick the current element <code>key = arr[i]</code>.</li>
    <li>Compare <code>key</code> with elements in the sorted part <code>arr[0..i-1]</code> from right to left.</li>
    <li>Shift all elements greater than <code>key</code> one position to the right.</li>
    <li>Insert <code>key</code> at the correct position.</li>
    <li>Repeat for all elements until the array is sorted.</li>
</ol>

<h2>Pseudocode</h2>
<pre>
for i = 1 to n-1
    key = arr[i]
    j = i - 1
    while j >= 0 and arr[j] > key
        arr[j+1] = arr[j]    // Shift element
        j = j - 1
    arr[j+1] = key          // Insert key
</pre>

<h2>Example</h2>
<p>Array: <code>[5, 2, 4, 6, 1, 3]</code></p>
<table>
<tr>
<th>Step</th>
<th>Sorted Part</th>
<th>Key</th>
<th>Array after insertion</th>
</tr>
<tr><td>1</td><td>[5]</td><td>2</td><td>[2, 5, 4, 6, 1, 3]</td></tr>
<tr><td>2</td><td>[2,5]</td><td>4</td><td>[2, 4, 5, 6, 1, 3]</td></tr>
<tr><td>3</td><td>[2,4,5]</td><td>6</td><td>[2, 4, 5, 6, 1, 3]</td></tr>
<tr><td>4</td><td>[2,4,5,6]</td><td>1</td><td>[1, 2, 4, 5, 6, 3]</td></tr>
<tr><td>5</td><td>[1,2,4,5,6]</td><td>3</td><td>[1, 2, 3, 4, 5, 6] ‚Üí Sorted</td></tr>
</table>

<h2>Complexity</h2>
<table>
<tr>
<th>Case</th>
<th>Time Complexity</th>
<th>Reason</th>
</tr>
<tr><td>Best Case</td><td>O(n)</td><td>Already sorted; only one comparison per element</td></tr>
<tr><td>Worst Case</td><td>O(n¬≤)</td><td>Reverse sorted; maximum shifts</td></tr>
<tr><td>Average Case</td><td>O(n¬≤)</td><td>Random order</td></tr>
<tr><td>Space</td><td>O(1)</td><td>In-place</td></tr>
<tr><td>Stability</td><td>Yes</td><td>Equal elements retain order</td></tr>
</table>

<h2>Key Points</h2>
<ul>
<li>Simple and easy to implement.</li>
<li>Efficient for small arrays or nearly sorted arrays.</li>
<li>Adaptive: Best case O(n) when the array is nearly sorted.</li>
<li>In-place and stable.</li>
</ul>

<h1>Selection Sort</h1>

<h2>üí° Idea</h2>
<p>Selection Sort works by repeatedly finding the minimum (or maximum) element from the unsorted part of the array and placing it at the beginning (or end).</p>
<p>It divides the array into:</p>
<ul>
    <li><strong>Sorted part:</strong> Left side</li>
    <li><strong>Unsorted part:</strong> Right side</li>
</ul>
<p>At each step, it picks the smallest element from the unsorted part and swaps it with the first element of the unsorted part.</p>

<h2>üìã Algorithm (Step by Step)</h2>
<p>Given Array: <code>[64, 25, 12, 22, 11]</code></p>

<pre>
Iteration 1:
Find minimum in [64, 25, 12, 22, 11] ‚Üí 11
Swap 11 with first element (64)
Array ‚Üí [11, 25, 12, 22, 64]

Iteration 2:
Find minimum in [25, 12, 22, 64] ‚Üí 12
Swap 12 with 25
Array ‚Üí [11, 12, 25, 22, 64]

Iteration 3:
Find minimum in [25, 22, 64] ‚Üí 22
Swap 22 with 25
Array ‚Üí [11, 12, 22, 25, 64]

Iteration 4:
Find minimum in [25, 64] ‚Üí 25
Already in correct position
Array ‚Üí [11, 12, 22, 25, 64]

Iteration 5:
Only one element left ‚Üí sorted
‚úÖ Sorted Array: [11, 12, 22, 25, 64]
</pre>

<h2>üìù Pseudocode</h2>
<pre>
for i = 0 to n-1:
    min_index = i
    for j = i+1 to n-1:
        if arr[j] < arr[min_index]:
            min_index = j
    swap arr[i] with arr[min_index]
</pre>

<h2>‚è± Complexity</h2>
<table>
<tr><th>Case</th><th>Time Complexity</th><th>Space Complexity</th><th>Stable?</th></tr>
<tr><td>Best Case</td><td>O(n¬≤)</td><td>O(1)</td><td>No</td></tr>
<tr><td>Worst Case</td><td>O(n¬≤)</td><td>O(1)</td><td>No</td></tr>
<tr><td>Average Case</td><td>O(n¬≤)</td><td>O(1)</td><td>No</td></tr>
</table>
<p>Note: Selection Sort always performs <code>n*(n-1)/2</code> comparisons ‚Üí O(n¬≤) for all cases.</p>
<p>In-place algorithm ‚Üí doesn‚Äôt require extra memory. Not stable ‚Üí equal elements may change order.</p>

<h2>üîë Key Points</h2>
<ul>
    <li>Simple, but inefficient for large arrays</li>
    <li>Useful for small datasets or when memory is very limited</li>
    <li>Fewer swaps than Bubble Sort ‚Üí better if writing to memory is costly</li>
</ul>

<h1>üîπ Merge Sort</h1>

<h2>1. Idea</h2>
<p>Merge Sort is a <strong>Divide and Conquer</strong> algorithm:</p>
<ul>
    <li><strong>Divide:</strong> Split the array into two halves.</li>
    <li><strong>Conquer:</strong> Recursively sort each half.</li>
    <li><strong>Combine (Merge):</strong> Merge the two sorted halves into a single sorted array.</li>
</ul>

<h2>2. Characteristics</h2>
<table>
<tr><th>Property</th><th>Details</th></tr>
<tr><td>Time Complexity</td><td>O(n log n) for all cases</td></tr>
<tr><td>Space Complexity</td><td>O(n) (temporary arrays for merging)</td></tr>
<tr><td>Stable</td><td>Yes (preserves relative order of equal elements)</td></tr>
<tr><td>Method</td><td>Recursive (can also be implemented iteratively)</td></tr>
</table>

<h2>3. How It Works (Step by Step)</h2>
<p><strong>Example Array:</strong> [38, 27, 43, 3]</p>

<h3>Divide</h3>
<pre>
[38, 27, 43, 3] ‚Üí [38, 27], [43, 3]
</pre>

<h3>Divide Again</h3>
<pre>
[38, 27] ‚Üí [38], [27]
[43, 3] ‚Üí [43], [3]
</pre>

<h3>Merge Sorted Halves</h3>
<pre>
[38], [27] ‚Üí [27, 38]
[43], [3] ‚Üí [3, 43]
</pre>

<h3>Merge Final Halves</h3>
<pre>
[27, 38], [3, 43] ‚Üí [3, 27, 38, 43]
</pre>

<p>‚úÖ <strong>Sorted Array:</strong> [3, 27, 38, 43]</p>

<h2>4. Merge Function (Pseudo-Code)</h2>
<pre>
merge(arr[], left[], right[], nL, nR) {
    i = j = k = 0;
    while(i &lt; nL &amp;&amp; j &lt; nR) {
        if(left[i] &lt;= right[j]) arr[k++] = left[i++];
        else arr[k++] = right[j++];
    }
    while(i &lt; nL) arr[k++] = left[i++];
    while(j &lt; nR) arr[k++] = right[j++];
}
</pre>

<h2>5. Merge Sort Function (Pseudo-Code)</h2>
<pre>
mergeSort(arr[], n) {
    if(n &lt; 2) return;
    mid = n/2;
    left[mid], right[n-mid];

    for(i=0; i&lt;mid; i++) left[i] = arr[i];
    for(i=mid; i&lt;n; i++) right[i-mid] = arr[i];

    mergeSort(left, mid);
    mergeSort(right, n-mid);

    merge(arr, left, right, mid, n-mid);
}
</pre>

<h2>6. Key Points</h2>
<ul>
    <li>Always split until size = 1, then merge.</li>
    <li>Requires extra space for temporary arrays.</li>
    <li>Works well for large datasets.</li>
    <li>Preferred when stable sorting is needed.</li>
</ul>
<h1>Quick Sort</h1>

<h2>1. Idea</h2>
<p>Quick Sort is a divide and conquer sorting algorithm:</p>
<ul>
    <li>Pick an element as pivot (first, last, random, or median).</li>
    <li>Partition the array:
        <ul>
            <li>Elements &lt; pivot ‚Üí left</li>
            <li>Elements &gt; pivot ‚Üí right</li>
        </ul>
    </li>
    <li>Recursively apply Quick Sort to the left and right subarrays.</li>
</ul>

<h2>2. Steps (Partitioning)</h2>
<ul>
    <li>Choose pivot (e.g., last element).</li>
    <li>Initialize two pointers:
        <ul>
            <li><strong>i</strong>: tracks position for smaller elements</li>
            <li><strong>j</strong>: iterates through the array</li>
        </ul>
    </li>
    <li>For each arr[j]:
        <ul>
            <li>If arr[j] &lt; pivot ‚Üí swap arr[i] and arr[j], increment i</li>
        </ul>
    </li>
    <li>After the loop, swap pivot with arr[i].</li>
    <li>Pivot is now at its correct sorted position.</li>
</ul>

<h2>3. Complexity</h2>
<table>
<tr>
<th>Case</th>
<th>Time Complexity</th>
</tr>
<tr><td>Best</td><td>O(n log n)</td></tr>
<tr><td>Average</td><td>O(n log n)</td></tr>
<tr><td>Worst</td><td>O(n¬≤) (pivot always min or max)</td></tr>
<tr><td>Space</td><td>O(log n) recursion</td></tr>
</table>
<p><strong>In-place:</strong> Yes (no extra array required)<br>
<strong>Stable:</strong> No</p>

<h2>4. Example</h2>
<p>Sort <code>[10, 7, 8, 9, 1, 5]</code></p>
<ul>
    <li>Step 1: Pivot = 5 ‚Üí Partition ‚Üí [1, 5, 10, 7, 8, 9]</li>
    <li>Step 2: Left [1] ‚Üí sorted, Right [10, 7, 8, 9], Pivot = 9 ‚Üí [7, 8, 9, 10]</li>
    <li>Step 3: Left [7, 8], Pivot = 8 ‚Üí [7, 8]</li>
    <li>Final Sorted Array: <code>[1, 5, 7, 8, 9, 10]</code></li>
</ul>

<h2>5. Quick Sort Pseudocode (C/Python-style)</h2>
<pre>
<code>
void quickSort(int arr[], int low, int high) {
    if (low &lt; high) {
        int pi = partition(arr, low, high); // partition index
        quickSort(arr, low, pi - 1);        // left subarray
        quickSort(arr, pi + 1, high);       // right subarray
    }
}

int partition(int arr[], int low, int high) {
    int pivot = arr[high];
    int i = low - 1;
    for (int j = low; j &lt; high; j++) {
        if (arr[j] &lt; pivot) {
            i++;
            swap(arr[i], arr[j]);
        }
    }
    swap(arr[i + 1], arr[high]);
    return i + 1;
}
</code>
</pre>

<h2>‚úÖ Key Points</h2>
<ul>
    <li>Efficient for large datasets</li>
    <li>Works in-place, but not stable</li>
    <li>Worst case can be avoided using randomized pivot selection</li>
</ul>

<h1>Heap Sort</h1>
<p>Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure. It is an improved selection sort that efficiently finds the maximum (or minimum) element using a heap. Typically, a Max-Heap is used to sort in ascending order.</p>

<h2>1. Key Concepts</h2>
<ul>
    <li><strong>Max-Heap:</strong> Binary tree where each parent ‚â• its children; largest element at the root.</li>
    <li><strong>Min-Heap:</strong> Binary tree where each parent ‚â§ its children; smallest element at the root.</li>
    <li>Heap properties allow efficient extraction of the largest or smallest element repeatedly.</li>
</ul>

<h2>2. Steps of Heap Sort</h2>
<ol>
    <li>Build a Max-Heap from the input array.</li>
    <li>Swap the root (maximum) with the last element of the heap.</li>
    <li>Reduce the heap size by 1 (ignore the last element now in correct position).</li>
    <li>Heapify the root to restore Max-Heap property.</li>
    <li>Repeat steps 2‚Äì4 until the heap size is 1.</li>
</ol>

<h2>3. Example</h2>
<p>Sort <code>[4, 10, 3, 5, 1]</code> in ascending order:</p>

<h3>Step 1: Build Max Heap</h3>
<pre>
Original: [4, 10, 3, 5, 1]
Max Heap: [10, 5, 3, 4, 1]
</pre>

<h3>Step 2: Swap root with last element</h3>
<pre>
Swap 10 and 1 ‚Üí [1, 5, 3, 4, 10]
Heapify root ‚Üí [5, 1, 3, 4, 10]
</pre>

<h3>Step 3: Repeat</h3>
<pre>
Swap 5 and 4 ‚Üí [4, 1, 3, 5, 10]
Heapify root ‚Üí [4, 1, 3, 5, 10]

Swap 4 and 3 ‚Üí [3, 1, 4, 5, 10]
Heapify root ‚Üí [3, 1, 4, 5, 10]

Swap 3 and 1 ‚Üí [1, 3, 4, 5, 10]
Heapify root ‚Üí [1, 3, 4, 5, 10]
</pre>

<p><strong>Final Sorted Array:</strong> <code>[1, 3, 4, 5, 10]</code></p>

<h2>4. Complexity</h2>
<table>
<tr><th>Aspect</th><th>Complexity</th></tr>
<tr><td>Time (Build Heap)</td><td>O(n)</td></tr>
<tr><td>Time (Heapify)</td><td>O(log n) per extraction ‚Üí O(n log n)</td></tr>
<tr><td>Overall Time</td><td>O(n log n)</td></tr>
<tr><td>Space</td><td>O(1)</td></tr>
<tr><td>Stable?</td><td>No</td></tr>
</table>

<h2>5. Advantages & Disadvantages</h2>
<h3>Advantages</h3>
<ul>
    <li>Efficient: O(n log n) in all cases</li>
    <li>In-place (no extra array needed)</li>
</ul>

<h3>Disadvantages</h3>
<ul>
    <li>Not stable (doesn‚Äôt preserve order of equal elements)</li>
    <li>Slightly slower than Quick Sort on average in practice</li>
</ul>

<h1>Sorting Algorithms</h1>

<table>
    <tr>
        <th>Algorithm</th>
        <th>Best Case</th>
        <th>Worst Case</th>
        <th>Average Case</th>
        <th>Space</th>
        <th>Stable?</th>
    </tr>
    <tr>
        <td>Bubble Sort</td>
        <td>O(n)</td>
        <td>O(n¬≤)</td>
        <td>O(n¬≤)</td>
        <td>O(1)</td>
        <td>Yes</td>
    </tr>
    <tr>
        <td>Insertion Sort</td>
        <td>O(n)</td>
        <td>O(n¬≤)</td>
        <td>O(n¬≤)</td>
        <td>O(1)</td>
        <td>Yes</td>
    </tr>
    <tr>
        <td>Selection Sort</td>
        <td>O(n¬≤)</td>
        <td>O(n¬≤)</td>
        <td>O(n¬≤)</td>
        <td>O(1)</td>
        <td>No</td>
    </tr>
    <tr>
        <td>Merge Sort</td>
        <td>O(n log n)</td>
        <td>O(n log n)</td>
        <td>O(n log n)</td>
        <td>O(n)</td>
        <td>Yes</td>
    </tr>
    <tr>
        <td>Quick Sort</td>
        <td>O(n log n)</td>
        <td>O(n¬≤)</td>
        <td>O(n log n)</td>
        <td>O(log n)</td>
        <td>No</td>
    </tr>
    <tr>
        <td>Heap Sort</td>
        <td>O(n log n)</td>
        <td>O(n log n)</td>
        <td>O(n log n)</td>
        <td>O(1)</td>
        <td>No</td>
    </tr>
</table>

<h2>‚úÖ Notes</h2>
<ul>
    <li><strong>Stable:</strong> Maintains relative order of equal elements.</li>
    <li><strong>Space Complexity:</strong> Merge sort requires extra space for merging; Heap sort and Quick sort (in-place versions) are more memory-efficient.</li>
    <li><strong>Quick Sort Worst Case:</strong> Occurs when pivot is always the smallest or largest element.</li>
    <li><strong>Bubble & Insertion Sort Best Case:</strong> Already sorted array ‚Üí O(n).</li>
</ul>

<h1>1. What is Recursion?</h1>
<p>Recursion is when a function calls itself to solve a smaller instance of the same problem.</p>

<h3>Key Points</h3>
<ul>
    <li>Must have a <strong>base case</strong> ‚Äî the condition where recursion stops.</li>
    <li>Must move toward the base case in each recursive call.</li>
    <li>Can be <strong>direct</strong> (function calls itself) or <strong>indirect</strong> (A calls B, B calls A).</li>
</ul>

<h2>2. Structure of a Recursive Function</h2>
<pre><code>return_type function_name(parameters) {
    if (base_case_condition) {
        return base_case_value;
    }
    // Recursive call
    return function_name(smaller_problem);
}</code></pre>

<h2>3. Examples</h2>

<h3>a) Factorial</h3>
<p>Factorial of n (denoted as n!) = n * (n-1) * (n-2) * ... * 1</p>
<p>Recursive formula:</p>
<pre><code>factorial(n) = n * factorial(n-1), factorial(0) = 1</code></pre>

<h4>C Code:</h4>
<pre><code>#include &lt;stdio.h&gt;

int factorial(int n) {
    if (n == 0)  // base case
        return 1;
    return n * factorial(n - 1); // recursive call
}

int main() {
    int n = 5;
    printf("Factorial of %d is %d\n", n, factorial(n));
    return 0;
}</code></pre>

<p><strong>Output:</strong></p>
<pre><code>Factorial of 5 is 120</code></pre>

<h3>b) Fibonacci Sequence</h3>
<p>Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13, ...</p>
<p>Each term = sum of the previous two terms.</p>
<p>Recursive formula:</p>
<pre><code>fib(n) = fib(n-1) + fib(n-2), fib(0) = 0, fib(1) = 1</code></pre>

<h4>C Code:</h4>
<pre><code>#include &lt;stdio.h&gt;

int fib(int n) {
    if (n == 0) return 0;  // base case
    if (n == 1) return 1;  // base case
    return fib(n-1) + fib(n-2);  // recursive call
}

int main() {
    int n = 7;
    printf("Fibonacci series up to %d terms:\n", n);
    for(int i = 0; i &lt; n; i++)
        printf("%d ", fib(i));
    return 0;
}</code></pre>

<p><strong>Output:</strong></p>
<pre><code>Fibonacci series up to 7 terms:
0 1 1 2 3 5 8</code></pre>

<h2>4. Visualization</h2>
<p>Example for <code>factorial(3)</code>:</p>
<pre><code>factorial(3)
-> 3 * factorial(2)
         -> 2 * factorial(1)
                  -> 1 * factorial(0)
                           -> 1 (base case)
Result: 3 * 2 * 1 * 1 = 6</code></pre>

<h3>üí° Tip</h3>
<p>Recursion is elegant but can be inefficient if repeated calculations occur (like Fibonacci). Use <strong>memoization</strong> or iteration to optimize.</p>
<h1>‚è± Time Complexity</h1>

<h2>1. What is Time Complexity?</h2>
<p>Time complexity measures the amount of time an algorithm takes to run as a function of the input size <strong>n</strong>.</p>
<ul>
    <li>Helps compare algorithms independent of hardware.</li>
    <li>Usually expressed in <strong>Big O</strong> notation: O(n), O(log n), etc.</li>
</ul>

<h2>2. Big O Notation</h2>
<p>Big O describes the <strong>upper bound</strong> of the running time of an algorithm.</p>

<table>
<tr>
<th>Complexity</th>
<th>Example</th>
<th>Description</th>
</tr>
<tr><td>O(1)</td><td>Accessing an array element</td><td>Constant time</td></tr>
<tr><td>O(log n)</td><td>Binary search</td><td>Logarithmic time</td></tr>
<tr><td>O(n)</td><td>Linear search</td><td>Linear time</td></tr>
<tr><td>O(n log n)</td><td>Merge sort, Quick sort (average)</td><td>Linearithmic time</td></tr>
<tr><td>O(n¬≤)</td><td>Bubble sort, Insertion sort</td><td>Quadratic time</td></tr>
<tr><td>O(2^n)</td><td>Recursive Fibonacci</td><td>Exponential time</td></tr>
</table>

<h1>1. What is Substitution Method?</h1>
<p>The Substitution Method is a technique to solve recurrence relations by <strong>guessing the solution</strong> and then proving it using <strong>mathematical induction</strong>.</p>

<h2>Steps:</h2>
<ol>
    <li>Guess the solution (T(n) = ?)</li>
    <li>Substitute it back into the recurrence</li>
    <li>Use mathematical induction to prove the guess is correct</li>
</ol>

<h2>2. Steps to Use Substitution Method</h2>

<h3>Step 1: Guess the Solution</h3>
<p>Look at the recurrence relation and estimate the growth rate.</p>
<pre>Example: For T(n) = 2T(n/2) + n, guess T(n) = O(n log n) (similar to Merge Sort)</pre>

<h3>Step 2: Substitute the Guess</h3>
<p>Replace T(n) in the recurrence with your guess and simplify. Check if it holds (possibly with constants).</p>

<h3>Step 3: Prove by Induction</h3>
<ul>
    <li>Show the inequality works for base case (n = 1)</li>
    <li>Assume it works for n = k</li>
    <li>Show it works for n = k+1 (or general n)</li>
</ul>

<h2>3. Examples</h2>

<h3>a) Merge Sort Recurrence</h3>
<pre>
Recurrence:
T(n) = 2T(n/2) + n

Step 1: Guess:
T(n) = O(n log n)

Step 2: Substitute:
T(n) ‚â§ 2 * c * (n/2) log(n/2) + n
       = c n log(n/2) + n
       = c n (log n - 1) + n
       = c n log n - c n + n

Step 3: Choose constant:
Choose c so that -c n + n ‚â§ 0
Then T(n) ‚â§ c n log n ‚úÖ

Result: T(n) = O(n log n)
</pre>

<h3>b) Simple Recursive Example</h3>
<pre>
Recurrence:
T(n) = T(n-1) + n

Step 1: Guess:
T(n) = O(n^2)

Step 2: Substitute:
T(n) = T(n-1) + n
     ‚â§ c (n-1)^2 + n
     = c (n^2 - 2n + 1) + n
     = c n^2 - 2 c n + c + n

Step 3: Choose constant:
Pick c ‚â• 1 ‚Üí inequality holds

‚úÖ So T(n) = O(n^2)
</pre>

<h2>4. Advantages of Substitution Method</h2>
<ul>
    <li>Works for most divide-and-conquer recurrences</li>
    <li>Can handle non-standard recurrences where Master Theorem doesn‚Äôt apply</li>
</ul>

<h2>üí° Tips:</h2>
<ul>
    <li>Always start with a guess based on the pattern</li>
    <li>Use mathematical induction carefully to adjust constants</li>
</ul>
<h1>üìò Master Theorem</h1>

<h2>1. What is Master Theorem?</h2>
<p>For a recurrence relation of the form:</p>
<pre>T(n) = a * T(n / b) + f(n)</pre>
<p>Where:</p>
<ul>
    <li><strong>n</strong> = problem size</li>
    <li><strong>a</strong> = number of subproblems</li>
    <li><strong>n/b</strong> = size of each subproblem</li>
    <li><strong>f(n)</strong> = cost to divide and combine (non-recursive work)</li>
</ul>
<p>Master Theorem provides the asymptotic behavior (Big O) of <code>T(n)</code>.</p>

<h2>2. The Three Cases</h2>

<h3>Case 1</h3>
<pre>f(n) = O(n^(log_b a - Œµ))</pre>
<p>Work done at leaves dominates.</p>
<pre>T(n) = Œò(n^(log_b a))</pre>

<h3>Case 2</h3>
<pre>f(n) = Œò(n^(log_b a))</pre>
<p>Work done at all levels is balanced.</p>
<pre>T(n) = Œò(n^(log_b a) * log n)</pre>

<h3>Case 3</h3>
<pre>f(n) = Œ©(n^(log_b a + Œµ))</pre>
<p>Work done at the root dominates.</p>
<pre>
T(n) = Œò(f(n))
Condition: a * f(n/b) ‚â§ c * f(n) for some c < 1 (regularity condition)
</pre>

<h2>3. Examples</h2>

<h3>a) Merge Sort</h3>
<pre>
Recurrence: T(n) = 2T(n/2) + O(n)
a = 2, b = 2, f(n) = n
n^(log_2 2) = n ‚Üí matches f(n) ‚Üí Case 2
Time Complexity: T(n) = O(n log n)
</pre>

<h3>b) Binary Search</h3>
<pre>
Recurrence: T(n) = T(n/2) + O(1)
a = 1, b = 2, f(n) = 1
n^(log_2 1) = n^0 = 1 ‚Üí matches f(n) ‚Üí Case 2
Time Complexity: T(n) = O(log n)
</pre>

<h3>c) Strassen's Matrix Multiplication</h3>
<pre>
Recurrence: T(n) = 7T(n/2) + O(n^2)
a = 7, b = 2, f(n) = n^2
n^(log_2 7) ‚âà n^2.81 ‚Üí f(n) grows slower ‚Üí Case 1
Time Complexity: T(n) = O(n^2.81)
</pre>

<h2>4. Quick Steps to Apply Master Theorem</h2>
<ol>
    <li>Identify <strong>a</strong>, <strong>b</strong>, and <strong>f(n)</strong> in the recurrence.</li>
    <li>Compute <code>n^(log_b a)</code>.</li>
    <li>Compare <code>f(n)</code> with <code>n^(log_b a)</code>:
        <ul>
            <li>f(n) &lt; n^(log_b a) ‚Üí Case 1</li>
            <li>f(n) = n^(log_b a) ‚Üí Case 2</li>
            <li>f(n) &gt; n^(log_b a) ‚Üí Case 3</li>
        </ul>
    </li>
    <li>Apply the formula for that case.</li>
</ol>
<p>üí° Tip: Always check the regularity condition for Case 3.</p>

</body>
</html>
